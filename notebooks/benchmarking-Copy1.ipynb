{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "import pickle as pic\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from collections import defaultdict\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    def __init__(self, char_list, num_states, parent=None, left=None, right=None):\n",
    "        # num_states includes the 0 state. For example, if the possible states are 0 or 1, then num_states=2\n",
    "        self.chars = char_list\n",
    "        self.parent = parent\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.num_chars = len(self.chars)\n",
    "        self.num_states = num_states\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return not (self.left or self.right)\n",
    "\n",
    "    def duplicate(self, p, q=None, dropout_rate=0):\n",
    "        assert len(p) == len(self.chars), \"invalid p vector\"\n",
    "        if q:\n",
    "            for i in range(len(p)):\n",
    "                assert len(q[i]) + 1 == self.num_states, \"invalid q[\" + str(i) + \"] vector\"\n",
    "                \n",
    "        new_chars = []\n",
    "        if not q:\n",
    "            q = [None for i in self.chars]\n",
    "\n",
    "        for l in range(len(self.chars)):\n",
    "            if self.chars[l] != 0:\n",
    "                new_chars.append(self.chars[l])\n",
    "            else:\n",
    "                if np.random.random() < p[l]:\n",
    "                    new_chars.append(np.random.choice(np.arange(1, self.num_states), 1, p=q[l])[0])\n",
    "                else:\n",
    "                    new_chars.append(self.chars[l])\n",
    "        \n",
    "        return Node(new_chars, self.num_states)\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        for x in self.chars:\n",
    "            s += str(x) + '|'\n",
    "        return s[:-1]\n",
    "                    \n",
    "def simulation(p, num_states, time, q=None):\n",
    "    root = Node([0 for i in p], num_states)\n",
    "    curr_gen = [root]\n",
    "    for t in range(time):\n",
    "        new_gen = []\n",
    "        for n in curr_gen:\n",
    "            c1 = n.duplicate(p, q)\n",
    "            c2 = n.duplicate(p, q)\n",
    "            c1.parent = n\n",
    "            c2.parent = n\n",
    "            n.left = c1\n",
    "            n.right = c2\n",
    "            new_gen.append(c1)\n",
    "            new_gen.append(c2)\n",
    "        curr_gen = new_gen\n",
    "    return curr_gen\n",
    "\n",
    "def find_lineage(node):\n",
    "    lineage = [node]\n",
    "    while node.parent:\n",
    "        node = node.parent\n",
    "        lineage.insert(0, node)\n",
    "    return lineage\n",
    "\n",
    "def find_root(samples):\n",
    "    return find_lineage(samples[0])[0]\n",
    "\n",
    "def print_tree(root):\n",
    "    \n",
    "    def tree_str(node, level=0):\n",
    "        ret = \"\\t\"*level+str(node)+\"\\n\"\n",
    "        if node.left:\n",
    "            ret += tree_str(node.left, level+1)\n",
    "        if node.right:\n",
    "            ret += tree_str(node.right, level+1)\n",
    "        return ret\n",
    "    \n",
    "    print(tree_str(root))\n",
    "    \n",
    "def generate_frequency_matrix(samples, subset=None):\n",
    "    k = samples[0].num_chars\n",
    "    m = samples[0].num_states + 1\n",
    "    F = np.zeros((k,m), dtype=int)\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    for i in subset:\n",
    "        for j in range(k):\n",
    "            F[j][samples[i].chars[j]] += 1\n",
    "    return F\n",
    "            \n",
    "def split_data(F):\n",
    "    k,m = F.shape[0], F.shape[1]\n",
    "    split_data = []\n",
    "    for i in range(k):\n",
    "        for j in range(1, m-1):\n",
    "            split_data.append((i,j))\n",
    "    split_data.sort(key=lambda tup: F[tup[0]][tup[1]], reverse=True)\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(5):\n",
    "        s = ''\n",
    "        for j in range(1, 5):\n",
    "            a, b = split_data[index][0], split_data[index][1]\n",
    "            s += str((a,b)) + \" freq =\" + str(F[a][b]) + \" \"\n",
    "            index += 1\n",
    "        print(s)\n",
    "            \n",
    "    \n",
    "def construct_connectivity_graph(samples, subset=None):\n",
    "    n = len(samples)\n",
    "    k = samples[0].num_chars\n",
    "    m = samples[0].num_states\n",
    "    G = nx.Graph()\n",
    "    if not subset:\n",
    "        subset = range(n)\n",
    "    for i in subset:\n",
    "        G.add_node(i)\n",
    "    F = generate_frequency_matrix(samples, subset)\n",
    "    for i in subset:\n",
    "        for j in subset:\n",
    "            if j <= i:\n",
    "                continue\n",
    "            n1 = samples[i]\n",
    "            n2 = samples[j]\n",
    "            #compute simularity score\n",
    "            score = 0\n",
    "            for l in range(k):\n",
    "                x = n1.chars[l]\n",
    "                y = n2.chars[l]\n",
    "                if min(x, y) >= 0 and max(x,y) > 0:\n",
    "                    if x==y:\n",
    "                        score -= 3*(len(subset) - F[l][x] - F[l][-1])\n",
    "                    elif min(x,y) == 0:\n",
    "                        score += F[l][max(x,y)] - 1\n",
    "                    else:\n",
    "                        score += (F[l][x] + F[l][y]) - 2\n",
    "                        \n",
    "                if score != 0:\n",
    "                    G.add_edge(i,j, weight=score)\n",
    "    return G\n",
    "\n",
    "def max_cut_heuristic(G, sdimension, iterations, show_steps=False):\n",
    "    #n = len(G.nodes())\n",
    "    d = sdimension+1\n",
    "    emb = {}        \n",
    "    for i in G.nodes():\n",
    "        x = np.random.normal(size=d)\n",
    "        x = x/np.linalg.norm(x)\n",
    "        emb[i] = x\n",
    "        \n",
    "    def show_relaxed_objective():\n",
    "        score = 0\n",
    "        for e in G.edges():\n",
    "            u = e[0]\n",
    "            v = e[1]\n",
    "            score += G[u][v]['weight']*np.linalg.norm(emb[u]-emb[v])\n",
    "        print(score)\n",
    "        \n",
    "    for k in range(iterations):\n",
    "        new_emb = {}\n",
    "        for i in G.nodes:\n",
    "            cm = np.zeros(d, dtype=float)\n",
    "            for j in G.neighbors(i):\n",
    "                cm -= G[i][j]['weight']*np.linalg.norm(emb[i]-emb[j])*emb[j]\n",
    "            cm = cm/np.linalg.norm(cm)\n",
    "            new_emb[i] = cm\n",
    "        emb = new_emb\n",
    "        \n",
    "    #print(\"final relaxed objective:\")\n",
    "    #show_relaxed_objective()\n",
    "    return_set = set()\n",
    "    best_score = 0\n",
    "    for k in range(3*d):\n",
    "        b = np.random.normal(size=d)\n",
    "        b = b/np.linalg.norm(b)\n",
    "        S = set()\n",
    "        for i in G.nodes():\n",
    "            if np.dot(emb[i], b) > 0:\n",
    "                S.add(i)\n",
    "        this_score = evaluate_cut(S, G)\n",
    "        if this_score > best_score:\n",
    "            return_set = S\n",
    "            best_score = this_score\n",
    "    #print(\"score before hill climb = \", best_score)\n",
    "    improved_S = improve_cut(G, return_set)\n",
    "    #final_score = evaluate_cut(improved_S, G)\n",
    "    #print(\"final score = \", final_score)\n",
    "    return improved_S\n",
    "\n",
    "def improve_cut(G, S):\n",
    "    #n = len(G.nodes())\n",
    "    ip = {}\n",
    "    new_S = S.copy()\n",
    "    for i in G.nodes():\n",
    "        improvement_potential = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if cut(i,j,new_S):\n",
    "                improvement_potential -= G[i][j]['weight']\n",
    "            else:\n",
    "                improvement_potential += G[i][j]['weight']\n",
    "        ip[i] = improvement_potential\n",
    "        \n",
    "    all_neg = False\n",
    "    iters = 0\n",
    "    while (not all_neg) and (iters < 2*len(G.nodes)):\n",
    "        best_potential = 0\n",
    "        best_index = 0\n",
    "        for i in G.nodes():\n",
    "            if ip[i] > best_potential:\n",
    "                best_potential = ip[i]\n",
    "                best_index = i\n",
    "        if best_potential > 0:\n",
    "            for j in G.neighbors(best_index):\n",
    "                if cut(best_index,j,new_S):\n",
    "                    ip[j] += 2*G[best_index][j]['weight']\n",
    "                else:\n",
    "                    ip[j] -= 2*G[best_index][j]['weight']\n",
    "            ip[best_index] = -ip[best_index]\n",
    "            if best_index in new_S:\n",
    "                new_S.remove(best_index)\n",
    "            else:\n",
    "                new_S.add(best_index)\n",
    "        else:\n",
    "            all_neg = True\n",
    "        iters += 1\n",
    "    #print(\"number of hill climbing interations: \", iters)\n",
    "    return new_S\n",
    "\n",
    "def evaluate_cut(S, G, B=None, show_total=False):\n",
    "    cut_score = 0\n",
    "    total_good = 0\n",
    "    total_bad = 0\n",
    "    for e in G.edges():\n",
    "        u = e[0]\n",
    "        v = e[1]\n",
    "        w_uv = G[u][v]['weight']\n",
    "        total_good += float(w_uv)\n",
    "        if cut(u,v,S):\n",
    "            cut_score += float(w_uv)\n",
    "\n",
    "    if B:\n",
    "        for e in B.edges():\n",
    "            u = e[0]\n",
    "            v = e[1]\n",
    "            w_uv = B[u][v]['weight']\n",
    "            total_bad += float(w_uv)\n",
    "            if cut(u,v,S):\n",
    "                cut_score -= float(w_uv)\n",
    "            \n",
    "    if show_total:\n",
    "        print(\"total good = \", total_good)\n",
    "        print(\"total bad = \", total_bad)\n",
    "    return(cut_score)\n",
    "\n",
    "def greedy_cut(samples, subset=None):\n",
    "    F = generate_frequency_matrix(samples, subset)\n",
    "    k,m = F.shape[0], F.shape[1]\n",
    "    freq = 0\n",
    "    char = 0\n",
    "    state = 0\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    for i in range(k):\n",
    "        for j in range(1, m-1):\n",
    "            if F[i][j] > freq and F[i][j] < len(subset) - F[i][-1] :\n",
    "                char, state = i,j\n",
    "                freq = F[i][j]\n",
    "    if freq == 0:\n",
    "        return random_nontrivial_cut(subset)\n",
    "    S = set()\n",
    "    Sc = set()\n",
    "    missing = set()\n",
    "    #print(char, state)\n",
    "    for i in subset:\n",
    "        if samples[i].chars[char] == state:\n",
    "            S.add(i)\n",
    "        elif samples[i].chars[char] == -1:\n",
    "            missing.add(i)\n",
    "        else:\n",
    "            Sc.add(i)\n",
    "            \n",
    "    if not Sc:\n",
    "        if len(S) == len(subset) or len(S) == 0:\n",
    "            print(F)\n",
    "            print(char, state, len(subset))\n",
    "        return S\n",
    "    \n",
    "    for i in missing:\n",
    "        s_score = 0\n",
    "        sc_score = 0\n",
    "        for j in S:\n",
    "            for l in range(k):\n",
    "                if samples[i].chars[l] > 0 and samples[i].chars[l] == samples[j].chars[l]:\n",
    "                    s_score += 1\n",
    "        for j in Sc:\n",
    "            for l in range(k):\n",
    "                if samples[i].chars[l] > 0  and samples[i].chars[l] == samples[j].chars[l]:\n",
    "                    sc_score += 1\n",
    "        if s_score/len(S) > sc_score/len(Sc):\n",
    "            S.add(i)\n",
    "        else:\n",
    "            Sc.add(i)\n",
    "        \n",
    "    if len(S) == len(subset) or len(S) == 0:\n",
    "            print(F)\n",
    "            print(char, state, len(subset))\n",
    "    return S\n",
    "    \n",
    "def random_cut(subset):\n",
    "    S = set()\n",
    "    for i in subset:\n",
    "        if np.random.random() > 0.5:\n",
    "            S.add(i)\n",
    "    return S\n",
    "\n",
    "def random_nontrivial_cut(subset):\n",
    "    assert len(subset) > 1\n",
    "    S = set()\n",
    "    lst = list(subset)\n",
    "    S.add(lst[0])\n",
    "    for i in range(2,len(lst)):\n",
    "        if np.random.random() > 0.5:\n",
    "            S.add(lst[i])\n",
    "    return S\n",
    "\n",
    "\n",
    "def cut(u, v, S):\n",
    "    return ((u in S) and (not v in S)) or ((v in S) and (not u in S))\n",
    "\n",
    "def num_incorrect(S, h):\n",
    "    num = 0\n",
    "    for i in range(int(2**h/2)):\n",
    "        if not i in S:\n",
    "            num += 1\n",
    "\n",
    "    for i in range(int(2**h/2), 2**h):\n",
    "        if i in S:\n",
    "            num += 1\n",
    "\n",
    "    return min(num, 2**h - num)\n",
    "\n",
    "def find_tree_lineage(i, T):\n",
    "    p = list(T.predecessors(i))\n",
    "    curr_node = i\n",
    "    ancestor_list = [curr_node]\n",
    "    while p:\n",
    "        curr_node = p[0]\n",
    "        ancestor_list.insert(0, curr_node)\n",
    "        p = list(T.predecessors(curr_node))\n",
    "    return(ancestor_list)\n",
    "\n",
    "        \n",
    "def outgroup(i, j, k, T):\n",
    "    assert i != j and i != k and j != k, str(i) + ' ' + str(j) + ' ' + str(k) + ' not distinct'\n",
    "    \n",
    "    Li = find_tree_lineage(i, T)\n",
    "    Lj = find_tree_lineage(j, T)\n",
    "    Lk = find_tree_lineage(k, T)\n",
    "    l = 0\n",
    "    while Li[l] == Lj[l] and Lj[l] == Lk[l]:\n",
    "        l += 1\n",
    "    if Li[l] != Lj[l] and Li[l] != Lk[l] and Lj[l] != Lk[l]:\n",
    "        return None\n",
    "    if Li[l] == Lj[l]:\n",
    "        return k\n",
    "    if Li[l] == Lk[l]:\n",
    "        return j\n",
    "    if Lj[l] == Lk[l]:\n",
    "        return i\n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate_split(S, subset, T, sample_size=1000):\n",
    "    # assume S \\subseteq T.leaves\n",
    "    def S_outgroup(i,j,k):\n",
    "        if (not cut(i,j,subset)) and (not cut(j,k,subset)):\n",
    "            return None\n",
    "        if not cut(i,j,subset):\n",
    "            return k\n",
    "        if not cut(i,k,subset):\n",
    "            return j\n",
    "        return i\n",
    "    \n",
    "    TC = 0\n",
    "    TI = 0\n",
    "    unresolved = 0\n",
    "    superset = np.array(list(S))\n",
    "    num_sampled = 0\n",
    "    for a in range(sample_size):\n",
    "        chosen = np.random.choice(superset, 3, replace=False)\n",
    "        oS = S_outgroup(chosen[0], chosen[1], chosen[2])\n",
    "        oT = outgroup(chosen[0], chosen[1], chosen[2], T)\n",
    "        if oS == None or oT == None:\n",
    "            unresolved += 1\n",
    "        else:\n",
    "            if oS == oT:\n",
    "                TC += 1\n",
    "            else:\n",
    "                TI += 1\n",
    "    return TC/sample_size, TI/sample_size, unresolved/sample_size\n",
    "                  \n",
    "def remove_duplicates(nodes, indices):\n",
    "    indices = list(indices)\n",
    "    indices.sort(key=lambda i: nodes[i].chars)\n",
    "    final_set = set()\n",
    "    i = 0\n",
    "    j = 1\n",
    "    while j < len(indices):\n",
    "        if nodes[indices[i]].chars != nodes[indices[j]].chars:\n",
    "            final_set.add(indices[i])\n",
    "            i = j\n",
    "        j += 1\n",
    "    final_set.add(indices[i])\n",
    "    return final_set\n",
    "            \n",
    "def mult_chain(a,b):\n",
    "    f = 1\n",
    "    for i in range(a, b+1):\n",
    "        f*=i\n",
    "    return f\n",
    "\n",
    "def nCr(n, k):\n",
    "    if k > n:\n",
    "        return 0\n",
    "    if k > n/2:\n",
    "        return nCr(n, n-k)\n",
    "    return int(mult_chain(n-k+1,n)/mult_chain(1,k))\n",
    "\n",
    "def similarity(u, v, samples):\n",
    "    k = samples[0].num_chars\n",
    "    return sum([1 for i in range(k) if samples[u].chars[i] == samples[v].chars[i] and samples[u].chars[i] > 0])\n",
    "\n",
    "def construct_similarity_graph(samples, subset=None, threshold=0):\n",
    "    G = nx.Graph()\n",
    "    if not subset:\n",
    "        subset = range(len(samples))\n",
    "    for i in subset:\n",
    "        G.add_node(i)\n",
    "    F = generate_frequency_matrix(samples, subset)\n",
    "    k,m = F.shape[0], F.shape[1]\n",
    "    for i in range(k):\n",
    "        for j in range(1,m-1):\n",
    "            if F[i][j] == len(subset) - F[i][-1]:\n",
    "                threshold += 1\n",
    "    for i in subset:\n",
    "        for j in subset:\n",
    "            if j <= i:\n",
    "                continue\n",
    "            s = similarity(i,j, samples) \n",
    "            if s > threshold:\n",
    "                G.add_edge(i,j, weight=(s-threshold))\n",
    "    return G\n",
    "\n",
    "def spectral_split(G, k=2, method='Fiedler', return_eig=False, display=False):\n",
    "    L = nx.normalized_laplacian_matrix(G).todense()\n",
    "    diag = sp.linalg.eig(L)\n",
    "    if k == 2 and method == 'Fiedler':\n",
    "        v2 = diag[1][:, 1] \n",
    "        x = {}\n",
    "        vertices = list(G.nodes())\n",
    "        for i in range(len(vertices)):\n",
    "            x[vertices[i]] = v2[i]\n",
    "        vertices.sort(key=lambda v: x[v])\n",
    "        total_weight = 2*sum([G[e[0]][e[1]]['weight'] for e in G.edges()])\n",
    "        S = set()\n",
    "        num = 0\n",
    "        denom = 0\n",
    "        best_score = 10000000\n",
    "        best_index = 0\n",
    "        for i in range(len(vertices) - 1):\n",
    "            v = vertices[i]\n",
    "            S.add(v)\n",
    "            cut_edges = 0\n",
    "            neighbor_weight = 0\n",
    "            for w in G.neighbors(v):\n",
    "                neighbor_weight += G[v][w]['weight']\n",
    "                if w in S:\n",
    "                    cut_edges += G[v][w]['weight']\n",
    "            denom += neighbor_weight\n",
    "            num += neighbor_weight - 2*cut_edges\n",
    "            if num == 0:\n",
    "                best_index = i\n",
    "                break\n",
    "            if num/min(denom, total_weight-denom) < best_score:\n",
    "                best_score = num/min(denom, total_weight-denom)\n",
    "                best_index = i\n",
    "        if display:\n",
    "            print(\"number of samples = \", len(v2))\n",
    "            print(\"lambda2 = \", diag[0][1])\n",
    "            plt.hist(v2, density=True, bins=30)\n",
    "            plt.hist([x[v] for v in vertices[:best_index+1]], density=True, bins=30)\n",
    "            plt.show()\n",
    "        if return_eig:\n",
    "            return vertices[:best_index+1], diag\n",
    "        return vertices[:best_index+1]\n",
    "\n",
    "def spectral_improve_cut(S, G, display=False):\n",
    "    delta_n = {}\n",
    "    delta_d = {}\n",
    "    ip = {}\n",
    "    new_S = set(S)\n",
    "    total_weight = 2*sum([G[e[0]][e[1]]['weight'] for e in G.edges()])\n",
    "    num =  sum([G[e[0]][e[1]]['weight'] for e in G.edges() if cut(e[0], e[1], new_S)])\n",
    "    denom = sum([sum([G[u][v]['weight'] for v in G.neighbors(u)]) for u in new_S])\n",
    "    if num == 0:\n",
    "        return list(new_S)\n",
    "    curr_score = num/min(denom, total_weight-denom)\n",
    "    \n",
    "    def set_ip(u):\n",
    "        if min(denom + delta_d[u], total_weight - denom - delta_d[u]) == 0:\n",
    "            ip[u] = 1000\n",
    "        else:\n",
    "            ip[u] = (num + delta_n[u])/min(denom + delta_d[u], total_weight - denom - delta_d[u]) - num/min(denom, total_weight - denom)\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        d = sum([G[u][v]['weight'] for v in G.neighbors(u)])\n",
    "        if d == 0:\n",
    "            return [u]\n",
    "        c = sum([G[u][v]['weight'] for v in G.neighbors(u) if cut(u,v,new_S)])\n",
    "        delta_n[u] = d-2*c\n",
    "        if u in new_S:\n",
    "            delta_d[u] = -d\n",
    "        else:\n",
    "            delta_d[u] = d\n",
    "        set_ip(u)\n",
    "    #TODO\n",
    "    all_neg = False\n",
    "    iters = 0\n",
    "    \n",
    "    while (not all_neg) and (iters < len(G.nodes)):\n",
    "        best_potential = 0\n",
    "        best_index = None\n",
    "        for v in G.nodes():\n",
    "            if ip[v] < best_potential:\n",
    "                best_potential = ip[v]\n",
    "                best_index = v\n",
    "        if not best_index is None:\n",
    "            num += delta_n[best_index]\n",
    "            denom += delta_d[best_index]\n",
    "            for j in G.neighbors(best_index):\n",
    "                if cut(best_index,j,new_S):\n",
    "                    delta_n[j] += 2*G[best_index][j]['weight']\n",
    "                else:\n",
    "                    delta_n[j] -= 2*G[best_index][j]['weight']\n",
    "                set_ip(j)\n",
    "            delta_n[best_index] = -delta_n[best_index]\n",
    "            delta_d[best_index] = -delta_d[best_index]\n",
    "            set_ip(best_index)\n",
    "            if best_index in new_S:\n",
    "                new_S.remove(best_index)\n",
    "            else:\n",
    "                new_S.add(best_index)\n",
    "            #print(\"curr scores:\", num/min(denom, total_weight - denom))\n",
    "        else:\n",
    "            all_neg = True\n",
    "        iters += 1\n",
    "    if display:\n",
    "        print(\"sgreed+ score, \",  num/min(denom, total_weight - denom))\n",
    "    return list(new_S)\n",
    "\n",
    "def evaluate_sparsity(S, G):\n",
    "    total_weight = 2*sum([G[e[0]][e[1]]['weight'] for e in G.edges()])\n",
    "    num =  sum([G[e[0]][e[1]]['weight'] for e in G.edges() if cut(e[0], e[1], S)])\n",
    "    denom = sum([sum([G[u][v]['weight'] for v in G.neighbors(u)]) for u in S])\n",
    "    return num/min(denom, total_weight - denom)\n",
    "    \n",
    "def build_tree_sep(samples, method='greedy', subset = None, sim_thresh=0, p = None, qs = None):\n",
    "    assert method in ['greedy', 'egreedy', 'SDP', 'greedy+', 'spectral', 'sgreedy+']\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    else:\n",
    "        subset = list(subset)\n",
    "    subset = remove_duplicates(samples, subset)\n",
    "    T = nx.DiGraph()\n",
    "    for i in subset:\n",
    "        T.add_node(i)\n",
    "    def build_helper(S):\n",
    "        assert S, \"error, S = \"+ str(S)\n",
    "        if len(S) == 1:\n",
    "            return list(S)[0]\n",
    "        left_set = set()\n",
    "        if method == 'greedy':\n",
    "            left_set = greedy_cut(samples, subset=S)\n",
    "        elif method == 'egreedy':\n",
    "            left_set = egreedy_cut(samples, p, qs, subset=S)\n",
    "        elif method == 'SDP':\n",
    "            G = construct_connectivity_graph(samples, subset=S)\n",
    "            left_set = max_cut_heuristic(G, 3, 50)\n",
    "        elif method == 'greedy+':\n",
    "            G = construct_connectivity_graph(samples, subset=S)\n",
    "            left_set = greedy_cut(samples, subset=S)\n",
    "            left_set = improve_cut(G,left_set)\n",
    "        elif method == 'spectral':\n",
    "            G = construct_similarity_graph(samples, subset=list(S), threshold=sim_thresh)\n",
    "            left_set = spectral_split(G)\n",
    "            left_set = spectral_improve_cut(left_set, G)\n",
    "        elif method == 'sgreedy+':\n",
    "            G = construct_similarity_graph(samples, subset=S, threshold=sim_thresh)\n",
    "            left_set = spectral_improve_cut(greedy_cut(samples, subset=S) , G)\n",
    "\n",
    "        if len(left_set) == 0 or len(left_set) == len(S):\n",
    "            left_set = greedy_cut(samples, subset=S)\n",
    "        right_set = set()\n",
    "        for i in S:\n",
    "            if not i in left_set:\n",
    "                right_set.add(i)\n",
    "        root = len(T.nodes) - len(subset) + len(samples)\n",
    "        T.add_node(root)\n",
    "        left_child = build_helper(left_set)\n",
    "        right_child = build_helper(right_set)\n",
    "        T.add_edge(root, left_child)\n",
    "        T.add_edge(root, right_child)\n",
    "        return root\n",
    "    build_helper(subset)\n",
    "    return T\n",
    "\n",
    "def triplets_correct_sep(T, Tt, sample_size=5000):\n",
    "    TC = 0\n",
    "    sample_set = np.array([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    for a in range(sample_size):\n",
    "        chosen = np.random.choice(sample_set, 3, replace=False)\n",
    "        if outgroup2(chosen[0], chosen[1], chosen[2], T)[0] == outgroup2(chosen[0], chosen[1], chosen[2], Tt)[0]:\n",
    "            TC += 1\n",
    "    return TC/sample_size\n",
    "\n",
    "def outgroup2(i, j, k, T):\n",
    "    assert i != j and i != k and j != k, str(i) + ' ' + str(j) + ' ' + str(k) + ' not distinct'\n",
    "    \n",
    "#     Li = find_tree_lineage(i, T)\n",
    "#     Lj = find_tree_lineage(j, T)\n",
    "#     Lk = find_tree_lineage(k, T)\n",
    "\n",
    "    Li = [node for node in nx.ancestors(T, i)]\n",
    "    Lj = [node for node in nx.ancestors(T, j)]\n",
    "    Lk = [node for node in nx.ancestors(T, k)]\n",
    "    \n",
    "    ij_common = len(set(Li) & set(Lj))\n",
    "    ik_common = len(set(Li) & set(Lk))\n",
    "    jk_common = len(set(Lj) & set(Lk))\n",
    "    index = min(ij_common, ik_common, jk_common)\n",
    "\n",
    "    if ij_common == ik_common and ik_common == jk_common:\n",
    "        return None, index\n",
    "    if ij_common > ik_common and ij_common > jk_common:\n",
    "        return k, index\n",
    "    elif jk_common > ik_common and jk_common > ij_common:\n",
    "        return i, index\n",
    "    elif ik_common > ij_common and ik_common > jk_common:\n",
    "        return j, index\n",
    "\n",
    "def triplets_correct_stratified(T, Tt, sample_size=5000, min_size_depth = 20):\n",
    "    correct_class = defaultdict(int)\n",
    "    freqs = defaultdict(int)\n",
    "    sample_set = np.array([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    \n",
    "    for a in range(sample_size):\n",
    "        chosen = np.random.choice(sample_set, 3, replace=False)\n",
    "        out1, index = outgroup2(chosen[0], chosen[1], chosen[2], T)\n",
    "        out2, index2 = outgroup2(chosen[0], chosen[1], chosen[2], Tt)\n",
    "        correct_class[index] += (out1 == out2)\n",
    "        freqs[index] += 1\n",
    "        \n",
    "    tot_tp = 0\n",
    "    num_consid = 0\n",
    "    \n",
    "    for k in correct_class.keys():\n",
    "        if freqs[k] > min_size_depth:\n",
    "\n",
    "            num_consid += 1\n",
    "            tot_tp += correct_class[k] / freqs[k]\n",
    "\n",
    "    tot_tp /= num_consid\n",
    "    return tot_tp\n",
    "\n",
    "def get_colless(network):\n",
    "    root = [n for n in network if network.in_degree(n) == 0][0]\n",
    "    colless = [0]\n",
    "    colless_helper(network, root, colless)\n",
    "    n = len([n for n in network if network.out_degree(n) == 0 and network.in_degree(n) == 1]) \n",
    "    return colless[0], (colless[0] - n * np.log(n) - n * (np.euler_gamma - 1 - np.log(2)))/n\n",
    "\n",
    "def colless_helper(network, node, colless):\n",
    "    if network.out_degree(node) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        leaves = []\n",
    "        for i in network.successors(node):\n",
    "            leaves.append(colless_helper(network, i, colless))\n",
    "        colless[0] += abs(leaves[0] - leaves[1])\n",
    "        return sum(leaves)\n",
    "\n",
    "def triplets_correct_at_time_sep(T, Tt, method='all', bin_size = 10, sample_size=5000, sampling_depths=None):\n",
    "    sample_set = set([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    children = {}\n",
    "    num_triplets = {}\n",
    "    nodes_at_depth = {}\n",
    "\n",
    "    def find_children(node, total_time):\n",
    "        t = total_time + Tt.nodes[node]['parent_lifespan']\n",
    "        children[node] = []\n",
    "        if Tt.out_degree(node) == 0:\n",
    "            if node in sample_set:\n",
    "                children[node].append(node)\n",
    "            return\n",
    "\n",
    "        for n in Tt.neighbors(node):\n",
    "            find_children(n, t)\n",
    "            children[node] += children[n]\n",
    "\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        num_triplets[node] = len(children[L])*nCr(len(children[R]), 2) + len(children[R])*nCr(len(children[L]), 2)\n",
    "        if num_triplets[node] > 0:\n",
    "            bin_num = t//bin_size\n",
    "            \n",
    "            if bin_num in nodes_at_depth:\n",
    "                nodes_at_depth[bin_num].append(node)\n",
    "            else:\n",
    "                nodes_at_depth[bin_num] = [node]\n",
    "                \n",
    "    root = [n for n in Tt if Tt.in_degree(n) == 0][0]\n",
    "    find_children(root, 0)\n",
    "\n",
    "    def sample_at_depth(d):\n",
    "        denom = sum([num_triplets[v] for v in nodes_at_depth[d]])\n",
    "        node = np.random.choice(nodes_at_depth[d], 1, [num_triplets[v]/denom for v in nodes_at_depth[d]])[0]\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        if np.random.random() < (len(children[R])-1)/(len(children[R])+len(children[L])-2):\n",
    "            outgrp = np.random.choice(children[L], 1)[0]\n",
    "            ingrp = np.random.choice(children[R], 2, replace=False)\n",
    "        else:\n",
    "            outgrp = np.random.choice(children[R], 1)[0]\n",
    "            ingrp = np.random.choice(children[L], 2, replace=False)\n",
    "        return outgroup(ingrp[0], ingrp[1], outgrp, T) == outgrp\n",
    "\n",
    "    if not sampling_depths:\n",
    "        sampling_depths = [d for d in range(len(nodes_at_depth))]\n",
    "    if method == 'aggregate':\n",
    "        score = 0\n",
    "        freq = 0\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    freq += 1\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "        return score/(sample_size*freq)\n",
    "    elif method == 'all':\n",
    "        ret = ['NA'] * len(sampling_depths)\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    score = 0\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "                    ret[d] = score/sample_size\n",
    "        return np.array(ret)\n",
    "\n",
    "def triplets_correct_at_depth_sep(T, Tt, method='all', sample_size=5000, sampling_depths=None):\n",
    "    sample_set = set([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    children = {}\n",
    "    num_triplets = {}\n",
    "    nodes_at_depth = {}\n",
    "\n",
    "    def find_children(node, depth):\n",
    "        children[node] = []\n",
    "        if Tt.out_degree(node) == 0:\n",
    "            if node in sample_set:\n",
    "                children[node].append(node)\n",
    "            return\n",
    "\n",
    "        for n in Tt.neighbors(node):\n",
    "            find_children(n, depth+1)\n",
    "            children[node] += children[n]\n",
    "\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        num_triplets[node] = len(children[L])*nCr(len(children[R]), 2) + len(children[R])*nCr(len(children[L]), 2)\n",
    "        if num_triplets[node] > 0:\n",
    "            if depth in nodes_at_depth:\n",
    "                nodes_at_depth[depth].append(node)\n",
    "            else:\n",
    "                nodes_at_depth[depth] = [node]\n",
    "                \n",
    "    root = [n for n in Tt if Tt.in_degree(n) == 0][0]\n",
    "    find_children(root, 0)\n",
    "\n",
    "    def sample_at_depth(d):\n",
    "        denom = sum([num_triplets[v] for v in nodes_at_depth[d]])\n",
    "        node = np.random.choice(nodes_at_depth[d], 1, [num_triplets[v]/denom for v in nodes_at_depth[d]])[0]\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        if np.random.random() < (len(children[R])-1)/(len(children[R])+len(children[L])-2):\n",
    "            outgrp = np.random.choice(children[L], 1)[0]\n",
    "            ingrp = np.random.choice(children[R], 2, replace=False)\n",
    "        else:\n",
    "            outgrp = np.random.choice(children[R], 1)[0]\n",
    "            ingrp = np.random.choice(children[L], 2, replace=False)\n",
    "        return outgroup(ingrp[0], ingrp[1], outgrp, T) == outgrp\n",
    "\n",
    "    if not sampling_depths:\n",
    "        sampling_depths = [d for d in range(len(nodes_at_depth))]\n",
    "        \n",
    "    if method == 'aggregate':\n",
    "        score = 0\n",
    "        freq = 0\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    freq += 1\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "        return score/(sample_size*freq)\n",
    "    elif method == 'all':\n",
    "        ret = ['NA'] * len(sampling_depths)\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    score = 0\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "                    ret[d] = score/sample_size\n",
    "        return np.array(ret) \n",
    "\n",
    "def eX(p, q, h):\n",
    "    if p == 0.5:\n",
    "        return 2*p*q*(h-1)\n",
    "    return (2*q*p*(1-(2-2*p)**h)/(2*p-1))\n",
    "\n",
    "def VarX(p, q, h):\n",
    "    def cross_terms(p, q, h):\n",
    "        if p == 0.5:\n",
    "            return p**2*q**2*sum([(h-d)**2 for d in range(h)])\n",
    "        s = 0\n",
    "        for d in range(h):\n",
    "            s += ((2-2*p)**d)*((2-2*p)**(h-d)-1)**2\n",
    "        return p**2*q**2*s/(2*p-1)**2\n",
    "    return 2*cross_terms(p,q,h) + eX(p,q,h) - eX(p,q,h)**2\n",
    "\n",
    "def eY(p, q, h):\n",
    "    return q*(1-(1-p)**h)\n",
    "\n",
    "def Var_Y(p,q,h):\n",
    "    Ey = q*(1-(1-p)**h)\n",
    "    s = 0\n",
    "    s += 2*(1-((1-p)/2)**h)/(1+p) - 4*(1-p)**h*(1-2**(-h)) + (2-2*p)*(1-p)**(2*h)*(1-(1/(2-2*p))**h)/(1-2*p)\n",
    "    Eyiyj = q**2*s/4\n",
    "    Eyiyi = p*q*(1-((1-p)/2)**h)/(1+p)\n",
    "    return 2*Eyiyj + Eyiyi - Ey**2\n",
    "\n",
    "def CoV(p, q, h):\n",
    "    s = 2*(2-2*p)**h*(1-2**(-h)) - (2-2*p)*(1-p)**h*((2-2*p)**h - 1)/(1-2*p) - (1-(1-p)**h)/p + h*(1-p)**h\n",
    "    twoExiyj = p*q**2*s/(1-2*p)\n",
    "    Ey = q*(1-(1-p)**h)\n",
    "    Ex = (2*q*p*(1-(2-2*p)**h)/(2*p-1))\n",
    "    return twoExiyj + Ey - Ex*Ey\n",
    "\n",
    "def generate_frequency_dict(samples, subset=None):\n",
    "    k = samples[0].num_chars\n",
    "    F = {}\n",
    "    for n in range(k):\n",
    "        F[n] = {}\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    for i in subset:\n",
    "        for j in range(k):\n",
    "            if samples[i].chars[j] in F[j]:\n",
    "                F[j][samples[i].chars[j]] += 1\n",
    "            else:\n",
    "                F[j][samples[i].chars[j]] = 1\n",
    "    return F\n",
    "\n",
    "def score_egreedy(samples, p, qs, subset = None):\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    F = generate_frequency_dict(samples, subset)\n",
    "    h = math.log2(len(subset))\n",
    "    total = 0\n",
    "    \n",
    "    if len(subset) > 1:\n",
    "        for char in F:\n",
    "            for state in F[char]:\n",
    "                if state == 0 or state == -1:\n",
    "                    continue\n",
    "                q = qs[char][str(state)]\n",
    "                if q > 0:\n",
    "                    total += (eX(p,q,h) + CoV(p,q,h)/Var_Y(p,q,h) * (F[char][state] - eY(p,q,h)))\n",
    "\n",
    "    return total\n",
    "\n",
    "def egreedy_cut(samples, p, qs, subset = None):\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    F = generate_frequency_dict(samples, subset)\n",
    "    k = samples[0].num_chars\n",
    "    min_score = 9223372036854775807\n",
    "    split_char = 0\n",
    "    split_state = 0\n",
    "    \n",
    "    for char in F:\n",
    "        for state in F[char]:\n",
    "            if state == 0 or state == -1:\n",
    "                continue\n",
    "                \n",
    "            S = set()\n",
    "            Sc = set()\n",
    "            missing = set()\n",
    "            #print(char, state)\n",
    "            for i in subset:\n",
    "                if samples[i].chars[char] == state:\n",
    "                    S.add(i)\n",
    "                elif samples[i].chars[char] == -1:\n",
    "                    missing.add(i)\n",
    "                else:\n",
    "                    Sc.add(i)\n",
    "            \n",
    "            if not Sc or not S:\n",
    "                continue\n",
    "                \n",
    "            score_S = score_egreedy(samples, p, qs, S)\n",
    "            score_Sc = score_egreedy(samples, p, qs, Sc)\n",
    "            \n",
    "#             print(score_S + score_Sc)\n",
    "            if score_S + score_Sc < min_score:\n",
    "                min_score = score_S + score_Sc\n",
    "                split_char = char\n",
    "                split_state = state\n",
    "    \n",
    "    if split_state == 0:\n",
    "        return random_nontrivial_cut(subset)\n",
    "    \n",
    "    S = set()\n",
    "    Sc = set()\n",
    "    missing = set()\n",
    "    #print(char, state)\n",
    "    for i in subset:\n",
    "        if samples[i].chars[split_char] == split_state:\n",
    "            S.add(i)\n",
    "        elif samples[i].chars[split_char] == -1:\n",
    "            missing.add(i)\n",
    "        else:\n",
    "            Sc.add(i)\n",
    "    \n",
    "    for i in missing:\n",
    "        s_score = 0\n",
    "        sc_score = 0\n",
    "        for j in S:\n",
    "            for l in range(k):\n",
    "                if samples[i].chars[l] > 0 and samples[i].chars[l] == samples[j].chars[l]:\n",
    "                    s_score += 1\n",
    "        for j in Sc:\n",
    "            for l in range(k):\n",
    "                if samples[i].chars[l] > 0  and samples[i].chars[l] == samples[j].chars[l]:\n",
    "                    sc_score += 1\n",
    "        if s_score/len(S) > sc_score/len(Sc):\n",
    "            S.add(i)\n",
    "        else:\n",
    "            Sc.add(i)\n",
    "            \n",
    "    if not Sc:\n",
    "        if len(S) == len(subset) or len(S) == 0:\n",
    "            print('One side of split empty')\n",
    "            print(F)\n",
    "            print(char, state, len(subset))\n",
    "        return S\n",
    "    \n",
    "#     if len(Sc) == 1 or len(S) == 1:\n",
    "#         print('split of size 1 chosen')\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6916333333333333 greedy\n",
      "1 0.7673090909090909 greedy\n",
      "2 0.7751666666666667 greedy\n",
      "3 0.7358545454545454 greedy\n",
      "4 0.7367428571428571 greedy\n",
      "5 0.7442857142857143 greedy\n",
      "6 0.7847 greedy\n",
      "7 0.7366833333333334 greedy\n",
      "8 0.7654285714285715 greedy\n",
      "9 0.7945636363636364 greedy\n",
      "0 0.5432166666666667 egreedy\n",
      "1 0.5462 egreedy\n",
      "2 0.6081833333333333 egreedy\n",
      "3 0.5560545454545455 egreedy\n",
      "4 0.5960285714285715 egreedy\n",
      "5 0.5807714285714286 egreedy\n",
      "6 0.56238 egreedy\n",
      "7 0.6355333333333333 egreedy\n",
      "8 0.5488571428571428 egreedy\n",
      "9 0.6350727272727272 egreedy\n",
      "0 0.6916833333333333 sgreedy+\n",
      "1 0.7698 sgreedy+\n",
      "2 0.7623666666666666 sgreedy+\n",
      "3 0.7524181818181818 sgreedy+\n",
      "4 0.7514857142857143 sgreedy+\n",
      "5 0.7543714285714286 sgreedy+\n",
      "6 0.77532 sgreedy+\n",
      "7 0.7272 sgreedy+\n",
      "8 0.7542857142857143 sgreedy+\n",
      "9 0.7937454545454545 sgreedy+\n",
      "0 0.4338666666666667 spectral\n",
      "1 0.3503636363636364 spectral\n",
      "2 0.5565166666666667 spectral\n",
      "3 0.5758545454545455 spectral\n",
      "4 0.5295714285714286 spectral\n",
      "5 0.32474285714285717 spectral\n",
      "6 0.53564 spectral\n",
      "7 0.5090666666666667 spectral\n",
      "8 0.5397 spectral\n",
      "9 0.5814363636363636 spectral\n",
      "0 0.73745 greedy+\n",
      "1 0.7668181818181818 greedy+\n",
      "2 0.7661166666666667 greedy+\n",
      "3 0.7852727272727272 greedy+\n",
      "4 0.7368 greedy+\n",
      "5 0.7556428571428572 greedy+\n",
      "6 0.80338 greedy+\n",
      "7 0.7114333333333334 greedy+\n",
      "8 0.7846142857142857 greedy+\n",
      "9 0.7840545454545454 greedy+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eecs/richardyz98/.local/lib/python3.6/site-packages/ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6910166666666666 SDP\n",
      "1 0.7699636363636364 SDP\n",
      "2 0.7637333333333334 SDP\n",
      "3 0.7449454545454546 SDP\n",
      "4 0.7214285714285714 SDP\n",
      "5 0.7541142857142857 SDP\n",
      "6 0.7739 SDP\n",
      "7 0.74015 SDP\n",
      "8 0.7852285714285714 SDP\n",
      "9 0.7601636363636364 SDP\n"
     ]
    }
   ],
   "source": [
    "# for drop in [10, 20, 30, 40, 50, 60]:\n",
    "# for drop in [0]:\n",
    "folder = \"400cells_base_nb\"\n",
    "path = \"/data/yosef2/users/richardz/projects/benchmarking/\" + folder + \"/\"\n",
    "nums = []\n",
    "#     triplets = []\n",
    "triplets_new = []\n",
    "colless = []\n",
    "types = []\n",
    "methods = []\n",
    "\n",
    "for method in [\"greedy\", \"egreedy\", \"sgreedy+\", \"spectral\", \"greedy+\", \"SDP\"]:\n",
    "    for num in range(0, 10):\n",
    "        dropout_cm = pd.read_csv(path + \"dropout_cm\" + str(num) + \".txt\", sep = '\\t', index_col = 0)\n",
    "        dropout_cm = dropout_cm.applymap(str)\n",
    "\n",
    "        samples = []\n",
    "        for index, row in dropout_cm.iterrows():\n",
    "            node = Node(list(row), 1001, parent=None, left=None, right=None)\n",
    "            samples.append(node)\n",
    "\n",
    "        subset = list(range(len(samples)))\n",
    "        prune_samples = remove_duplicates(samples, subset)\n",
    "\n",
    "        node_map = {}\n",
    "        for i in prune_samples:\n",
    "            node_map[i] = list(dropout_cm.iloc[i,:])\n",
    "\n",
    "        net = pic.load(open(path + \"dropout_net\" + str(num) + \".pkl\", 'rb'))\n",
    "        ground = net.network\n",
    "        leaves = [n for n in ground if ground.out_degree(n) == 0 and ground.in_degree(n) == 1]\n",
    "        ground_net_map = {}\n",
    "        for i in node_map:\n",
    "            for j in leaves:\n",
    "                if node_map[i] == j.char_vec:\n",
    "                    ground_net_map[j] = i\n",
    "                    break\n",
    "        ground = nx.relabel_nodes(ground, ground_net_map)\n",
    "\n",
    "        dropout_cm = dropout_cm.replace(\"-\", -1)\n",
    "        dropout_cm = dropout_cm.replace(\"*\", -1)\n",
    "        dropout_cm = dropout_cm.apply(pd.to_numeric)\n",
    "\n",
    "        samples = []\n",
    "        for index, row in dropout_cm.iterrows():\n",
    "            node = Node(list(row), 1001, parent=None, left=None, right=None)\n",
    "            samples.append(node)\n",
    "    #     sample_map = {}\n",
    "    #     for i in range(len(samples)):\n",
    "    #         sample_map[i] = samples[i]\n",
    "        if method == 'egreedy':\n",
    "            p = 0.007550225842780328\n",
    "#                 p = np.log(1 - (mut/100))/(-1 * 100)\n",
    "            priors = pic.load(open(path + \"priors\" + str(num) + \".pkl\", 'rb'))\n",
    "            recon = build_tree_sep(samples, method=\"egreedy\", p = p, qs = priors)\n",
    "        else:\n",
    "            recon = build_tree_sep(samples, method=method)\n",
    "#             trip = triplets_correct_stratified(recon, ground)\n",
    "        trip2 = triplets_correct_at_depth_sep(recon, ground, 'aggregate')\n",
    "        print(num, trip2, method)\n",
    "#             triplets.append(trip)\n",
    "        triplets_new.append(trip2)\n",
    "        nums.append(num)\n",
    "        colless.append(get_colless(ground)[0])\n",
    "        types.append(folder)\n",
    "        methods.append(method)\n",
    "\n",
    "data = [nums, triplets_new, colless, methods, types]\n",
    "df = pd.DataFrame(data)\n",
    "df = df.T\n",
    "df = df.rename(columns = {0: 'Run', 1: 'TripletsCorrect', 2:'Colless', 3:'Method', 4: 'Type'})\n",
    "df.to_csv(path + 'methods_triplets2.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "17\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "17\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "17\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "17\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "17\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "15\n",
      "16\n",
      "17\n",
      "17\n",
      "17\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(n) for n in triplets_new])\n",
    "for i in range(len(triplets_new)):\n",
    "    triplets_new[i]\n",
    "    while(len(triplets_new[i]) < max_len):\n",
    "        print(len(triplets_new[i]))\n",
    "        triplets_new[i] = np.append(triplets_new[i], ['NA'], axis = 0)\n",
    "df = pd.DataFrame(triplets_new)\n",
    "# [len(n) for n in triplets_new]\n",
    "df.to_csv(path + 'methods_triplets_depth.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/yosef2/users/richardz/projects/benchmarking/400cells_base_m/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
