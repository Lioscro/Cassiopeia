{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "import pickle as pic\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from cassiopeia.TreeSolver.Cassiopeia_Tree import Cassiopeia_Tree\n",
    "from cassiopeia.TreeSolver.Node import Node as Cass_Node\n",
    "from collections import defaultdict\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    def __init__(self, char_list, num_states, parent=None, left=None, right=None):\n",
    "        # num_states includes the 0 state. For example, if the possible states are 0 or 1, then num_states=2\n",
    "        self.chars = char_list\n",
    "        self.parent = parent\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.num_chars = len(self.chars)\n",
    "        self.num_states = num_states\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return not (self.left or self.right)\n",
    "\n",
    "    def duplicate(self, p, q=None, dropout_rate=0):\n",
    "        assert len(p) == len(self.chars), \"invalid p vector\"\n",
    "        if q:\n",
    "            for i in range(len(p)):\n",
    "                assert len(q[i]) + 1 == self.num_states, \"invalid q[\" + str(i) + \"] vector\"\n",
    "                \n",
    "        new_chars = []\n",
    "        if not q:\n",
    "            q = [None for i in self.chars]\n",
    "\n",
    "        for l in range(len(self.chars)):\n",
    "            if self.chars[l] != 0:\n",
    "                new_chars.append(self.chars[l])\n",
    "            else:\n",
    "                if np.random.random() < p[l]:\n",
    "                    new_chars.append(np.random.choice(np.arange(1, self.num_states), 1, p=q[l])[0])\n",
    "                else:\n",
    "                    new_chars.append(self.chars[l])\n",
    "        \n",
    "        return Node(new_chars, self.num_states)\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        for x in self.chars:\n",
    "            s += str(x) + '|'\n",
    "        return s[:-1]\n",
    "                    \n",
    "def simulation(p, num_states, time, q=None):\n",
    "    root = Node([0 for i in p], num_states)\n",
    "    curr_gen = [root]\n",
    "    for t in range(time):\n",
    "        new_gen = []\n",
    "        for n in curr_gen:\n",
    "            c1 = n.duplicate(p, q)\n",
    "            c2 = n.duplicate(p, q)\n",
    "            c1.parent = n\n",
    "            c2.parent = n\n",
    "            n.left = c1\n",
    "            n.right = c2\n",
    "            new_gen.append(c1)\n",
    "            new_gen.append(c2)\n",
    "        curr_gen = new_gen\n",
    "    return curr_gen\n",
    "\n",
    "def find_lineage(node):\n",
    "    lineage = [node]\n",
    "    while node.parent:\n",
    "        node = node.parent\n",
    "        lineage.insert(0, node)\n",
    "    return lineage\n",
    "\n",
    "def find_root(samples):\n",
    "    return find_lineage(samples[0])[0]\n",
    "\n",
    "def print_tree(root):\n",
    "    \n",
    "    def tree_str(node, level=0):\n",
    "        ret = \"\\t\"*level+str(node)+\"\\n\"\n",
    "        if node.left:\n",
    "            ret += tree_str(node.left, level+1)\n",
    "        if node.right:\n",
    "            ret += tree_str(node.right, level+1)\n",
    "        return ret\n",
    "    \n",
    "    print(tree_str(root))\n",
    "    \n",
    "def generate_frequency_matrix(samples, subset=None):\n",
    "    k = samples[0].num_chars\n",
    "    m = samples[0].num_states + 1\n",
    "    F = np.zeros((k,m), dtype=int)\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    for i in subset:\n",
    "        for j in range(k):\n",
    "            F[j][samples[i].chars[j]] += 1\n",
    "    return F\n",
    "            \n",
    "def split_data(F):\n",
    "    k,m = F.shape[0], F.shape[1]\n",
    "    split_data = []\n",
    "    for i in range(k):\n",
    "        for j in range(1, m-1):\n",
    "            split_data.append((i,j))\n",
    "    split_data.sort(key=lambda tup: F[tup[0]][tup[1]], reverse=True)\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(5):\n",
    "        s = ''\n",
    "        for j in range(1, 5):\n",
    "            a, b = split_data[index][0], split_data[index][1]\n",
    "            s += str((a,b)) + \" freq =\" + str(F[a][b]) + \" \"\n",
    "            index += 1\n",
    "        print(s)\n",
    "            \n",
    "    \n",
    "def construct_connectivity_graph(samples, subset=None):\n",
    "    n = len(samples)\n",
    "    k = samples[0].num_chars\n",
    "    m = samples[0].num_states\n",
    "    G = nx.Graph()\n",
    "    if not subset:\n",
    "        subset = range(n)\n",
    "    for i in subset:\n",
    "        G.add_node(i)\n",
    "    F = generate_frequency_matrix(samples, subset)\n",
    "    for i in subset:\n",
    "        for j in subset:\n",
    "            if j <= i:\n",
    "                continue\n",
    "            n1 = samples[i]\n",
    "            n2 = samples[j]\n",
    "            #compute simularity score\n",
    "            score = 0\n",
    "            for l in range(k):\n",
    "                x = n1.chars[l]\n",
    "                y = n2.chars[l]\n",
    "                if min(x, y) >= 0 and max(x,y) > 0:\n",
    "                    if x==y:\n",
    "                        score -= 3*(len(subset) - F[l][x] - F[l][-1])\n",
    "                    elif min(x,y) == 0:\n",
    "                        score += F[l][max(x,y)] - 1\n",
    "                    else:\n",
    "                        score += (F[l][x] + F[l][y]) - 2\n",
    "                        \n",
    "                if score != 0:\n",
    "                    G.add_edge(i,j, weight=score)\n",
    "    return G\n",
    "\n",
    "def max_cut_heuristic(G, sdimension, iterations, show_steps=False):\n",
    "    #n = len(G.nodes())\n",
    "    d = sdimension+1\n",
    "    emb = {}        \n",
    "    for i in G.nodes():\n",
    "        x = np.random.normal(size=d)\n",
    "        x = x/np.linalg.norm(x)\n",
    "        emb[i] = x\n",
    "        \n",
    "    def show_relaxed_objective():\n",
    "        score = 0\n",
    "        for e in G.edges():\n",
    "            u = e[0]\n",
    "            v = e[1]\n",
    "            score += G[u][v]['weight']*np.linalg.norm(emb[u]-emb[v])\n",
    "        print(score)\n",
    "        \n",
    "    for k in range(iterations):\n",
    "        new_emb = {}\n",
    "        for i in G.nodes:\n",
    "            cm = np.zeros(d, dtype=float)\n",
    "            for j in G.neighbors(i):\n",
    "                cm -= G[i][j]['weight']*np.linalg.norm(emb[i]-emb[j])*emb[j]\n",
    "            cm = cm/np.linalg.norm(cm)\n",
    "            new_emb[i] = cm\n",
    "        emb = new_emb\n",
    "        \n",
    "    #print(\"final relaxed objective:\")\n",
    "    #show_relaxed_objective()\n",
    "    return_set = set()\n",
    "    best_score = 0\n",
    "    for k in range(3*d):\n",
    "        b = np.random.normal(size=d)\n",
    "        b = b/np.linalg.norm(b)\n",
    "        S = set()\n",
    "        for i in G.nodes():\n",
    "            if np.dot(emb[i], b) > 0:\n",
    "                S.add(i)\n",
    "        this_score = evaluate_cut(S, G)\n",
    "        if this_score > best_score:\n",
    "            return_set = S\n",
    "            best_score = this_score\n",
    "    #print(\"score before hill climb = \", best_score)\n",
    "    improved_S = improve_cut(G, return_set)\n",
    "    #final_score = evaluate_cut(improved_S, G)\n",
    "    #print(\"final score = \", final_score)\n",
    "    return improved_S\n",
    "\n",
    "def improve_cut(G, S):\n",
    "    #n = len(G.nodes())\n",
    "    ip = {}\n",
    "    new_S = S.copy()\n",
    "    for i in G.nodes():\n",
    "        improvement_potential = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if cut(i,j,new_S):\n",
    "                improvement_potential -= G[i][j]['weight']\n",
    "            else:\n",
    "                improvement_potential += G[i][j]['weight']\n",
    "        ip[i] = improvement_potential\n",
    "        \n",
    "    all_neg = False\n",
    "    iters = 0\n",
    "    while (not all_neg) and (iters < 2*len(G.nodes)):\n",
    "        best_potential = 0\n",
    "        best_index = 0\n",
    "        for i in G.nodes():\n",
    "            if ip[i] > best_potential:\n",
    "                best_potential = ip[i]\n",
    "                best_index = i\n",
    "        if best_potential > 0:\n",
    "            for j in G.neighbors(best_index):\n",
    "                if cut(best_index,j,new_S):\n",
    "                    ip[j] += 2*G[best_index][j]['weight']\n",
    "                else:\n",
    "                    ip[j] -= 2*G[best_index][j]['weight']\n",
    "            ip[best_index] = -ip[best_index]\n",
    "            if best_index in new_S:\n",
    "                new_S.remove(best_index)\n",
    "            else:\n",
    "                new_S.add(best_index)\n",
    "        else:\n",
    "            all_neg = True\n",
    "        iters += 1\n",
    "    #print(\"number of hill climbing interations: \", iters)\n",
    "    return new_S\n",
    "\n",
    "def evaluate_cut(S, G, B=None, show_total=False):\n",
    "    cut_score = 0\n",
    "    total_good = 0\n",
    "    total_bad = 0\n",
    "    for e in G.edges():\n",
    "        u = e[0]\n",
    "        v = e[1]\n",
    "        w_uv = G[u][v]['weight']\n",
    "        total_good += float(w_uv)\n",
    "        if cut(u,v,S):\n",
    "            cut_score += float(w_uv)\n",
    "\n",
    "    if B:\n",
    "        for e in B.edges():\n",
    "            u = e[0]\n",
    "            v = e[1]\n",
    "            w_uv = B[u][v]['weight']\n",
    "            total_bad += float(w_uv)\n",
    "            if cut(u,v,S):\n",
    "                cut_score -= float(w_uv)\n",
    "            \n",
    "    if show_total:\n",
    "        print(\"total good = \", total_good)\n",
    "        print(\"total bad = \", total_bad)\n",
    "    return(cut_score)\n",
    "\n",
    "def greedy_cut(samples, subset=None):\n",
    "    F = generate_frequency_matrix(samples, subset)\n",
    "    k,m = F.shape[0], F.shape[1]\n",
    "    freq = 0\n",
    "    char = 0\n",
    "    state = 0\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    for i in range(k):\n",
    "        for j in range(1, m-1):\n",
    "            if F[i][j] > freq and F[i][j] < len(subset) - F[i][-1] :\n",
    "                char, state = i,j\n",
    "                freq = F[i][j]\n",
    "    if freq == 0:\n",
    "        return random_nontrivial_cut(subset)\n",
    "    S = set()\n",
    "    Sc = set()\n",
    "    missing = set()\n",
    "    #print(char, state)\n",
    "    for i in subset:\n",
    "        if samples[i].chars[char] == state:\n",
    "            S.add(i)\n",
    "        elif samples[i].chars[char] == -1:\n",
    "            missing.add(i)\n",
    "        else:\n",
    "            Sc.add(i)\n",
    "            \n",
    "    if not Sc:\n",
    "        if len(S) == len(subset) or len(S) == 0:\n",
    "            print(F)\n",
    "            print(char, state, len(subset))\n",
    "        return S\n",
    "    \n",
    "    for i in missing:\n",
    "        s_score = 0\n",
    "        sc_score = 0\n",
    "        for j in S:\n",
    "            for l in range(k):\n",
    "                if samples[i].chars[l] > 0 and samples[i].chars[l] == samples[j].chars[l]:\n",
    "                    s_score += 1\n",
    "        for j in Sc:\n",
    "            for l in range(k):\n",
    "                if samples[i].chars[l] > 0  and samples[i].chars[l] == samples[j].chars[l]:\n",
    "                    sc_score += 1\n",
    "        if s_score/len(S) > sc_score/len(Sc):\n",
    "            S.add(i)\n",
    "        else:\n",
    "            Sc.add(i)\n",
    "        \n",
    "    if len(S) == len(subset) or len(S) == 0:\n",
    "            print(F)\n",
    "            print(char, state, len(subset))\n",
    "    return S\n",
    "    \n",
    "def random_cut(subset):\n",
    "    S = set()\n",
    "    for i in subset:\n",
    "        if np.random.random() > 0.5:\n",
    "            S.add(i)\n",
    "    return S\n",
    "\n",
    "def random_nontrivial_cut(subset):\n",
    "    assert len(subset) > 1\n",
    "    S = set()\n",
    "    lst = list(subset)\n",
    "    S.add(lst[0])\n",
    "    for i in range(2,len(lst)):\n",
    "        if np.random.random() > 0.5:\n",
    "            S.add(lst[i])\n",
    "    return S\n",
    "\n",
    "\n",
    "def cut(u, v, S):\n",
    "    return ((u in S) and (not v in S)) or ((v in S) and (not u in S))\n",
    "\n",
    "def num_incorrect(S, h):\n",
    "    num = 0\n",
    "    for i in range(int(2**h/2)):\n",
    "        if not i in S:\n",
    "            num += 1\n",
    "\n",
    "    for i in range(int(2**h/2), 2**h):\n",
    "        if i in S:\n",
    "            num += 1\n",
    "\n",
    "    return min(num, 2**h - num)\n",
    "\n",
    "def find_tree_lineage(i, T):\n",
    "    p = list(T.predecessors(i))\n",
    "    curr_node = i\n",
    "    ancestor_list = [curr_node]\n",
    "    while p:\n",
    "        curr_node = p[0]\n",
    "        ancestor_list.insert(0, curr_node)\n",
    "        p = list(T.predecessors(curr_node))\n",
    "    return(ancestor_list)\n",
    "\n",
    "        \n",
    "def outgroup(i, j, k, T):\n",
    "    assert i != j and i != k and j != k, str(i) + ' ' + str(j) + ' ' + str(k) + ' not distinct'\n",
    "    \n",
    "    Li = find_tree_lineage(i, T)\n",
    "    Lj = find_tree_lineage(j, T)\n",
    "    Lk = find_tree_lineage(k, T)\n",
    "    l = 0\n",
    "    while Li[l] == Lj[l] and Lj[l] == Lk[l]:\n",
    "        l += 1\n",
    "    if Li[l] != Lj[l] and Li[l] != Lk[l] and Lj[l] != Lk[l]:\n",
    "        return None\n",
    "    if Li[l] == Lj[l]:\n",
    "        return k\n",
    "    if Li[l] == Lk[l]:\n",
    "        return j\n",
    "    if Lj[l] == Lk[l]:\n",
    "        return i\n",
    "    \n",
    "def remove_duplicates(nodes, indices):\n",
    "    indices = list(indices)\n",
    "    indices.sort(key=lambda i: nodes[i].chars)\n",
    "    final_set = set()\n",
    "    i = 0\n",
    "    j = 1\n",
    "    while j < len(indices):\n",
    "        if nodes[indices[i]].chars != nodes[indices[j]].chars:\n",
    "            final_set.add(indices[i])\n",
    "            i = j\n",
    "        j += 1\n",
    "    final_set.add(indices[i])\n",
    "    return final_set\n",
    "            \n",
    "def mult_chain(a,b):\n",
    "    f = 1\n",
    "    for i in range(a, b+1):\n",
    "        f*=i\n",
    "    return f\n",
    "\n",
    "def nCr(n, k):\n",
    "    if k > n:\n",
    "        return 0\n",
    "    if k > n/2:\n",
    "        return nCr(n, n-k)\n",
    "    return int(mult_chain(n-k+1,n)/mult_chain(1,k))\n",
    "\n",
    "def similarity(u, v, samples):\n",
    "    k = samples[0].num_chars\n",
    "    return sum([1 for i in range(k) if samples[u].chars[i] == samples[v].chars[i] and samples[u].chars[i] > 0])\n",
    "\n",
    "def construct_similarity_graph(samples, subset=None, threshold=0):\n",
    "    G = nx.Graph()\n",
    "    if not subset:\n",
    "        subset = range(len(samples))\n",
    "    for i in subset:\n",
    "        G.add_node(i)\n",
    "    F = generate_frequency_matrix(samples, subset)\n",
    "    k,m = F.shape[0], F.shape[1]\n",
    "    for i in range(k):\n",
    "        for j in range(1,m-1):\n",
    "            if F[i][j] == len(subset) - F[i][-1]:\n",
    "                threshold += 1\n",
    "    for i in subset:\n",
    "        for j in subset:\n",
    "            if j <= i:\n",
    "                continue\n",
    "            s = similarity(i,j, samples) \n",
    "            if s > threshold:\n",
    "                G.add_edge(i,j, weight=(s-threshold))\n",
    "    return G\n",
    "\n",
    "def spectral_split(G, k=2, method='Fiedler', return_eig=False, display=False):\n",
    "    L = nx.normalized_laplacian_matrix(G).todense()\n",
    "    diag = sp.linalg.eig(L)\n",
    "    if k == 2 and method == 'Fiedler':\n",
    "        v2 = diag[1][:, 1] \n",
    "        x = {}\n",
    "        vertices = list(G.nodes())\n",
    "        for i in range(len(vertices)):\n",
    "            x[vertices[i]] = v2[i]\n",
    "        vertices.sort(key=lambda v: x[v])\n",
    "        total_weight = 2*sum([G[e[0]][e[1]]['weight'] for e in G.edges()])\n",
    "        S = set()\n",
    "        num = 0\n",
    "        denom = 0\n",
    "        best_score = 10000000\n",
    "        best_index = 0\n",
    "        for i in range(len(vertices) - 1):\n",
    "            v = vertices[i]\n",
    "            S.add(v)\n",
    "            cut_edges = 0\n",
    "            neighbor_weight = 0\n",
    "            for w in G.neighbors(v):\n",
    "                neighbor_weight += G[v][w]['weight']\n",
    "                if w in S:\n",
    "                    cut_edges += G[v][w]['weight']\n",
    "            denom += neighbor_weight\n",
    "            num += neighbor_weight - 2*cut_edges\n",
    "            if num == 0:\n",
    "                best_index = i\n",
    "                break\n",
    "            if num/min(denom, total_weight-denom) < best_score:\n",
    "                best_score = num/min(denom, total_weight-denom)\n",
    "                best_index = i\n",
    "        if display:\n",
    "            print(\"number of samples = \", len(v2))\n",
    "            print(\"lambda2 = \", diag[0][1])\n",
    "            plt.hist(v2, density=True, bins=30)\n",
    "            plt.hist([x[v] for v in vertices[:best_index+1]], density=True, bins=30)\n",
    "            plt.show()\n",
    "        if return_eig:\n",
    "            return vertices[:best_index+1], diag\n",
    "        return vertices[:best_index+1]\n",
    "\n",
    "def spectral_improve_cut(S, G, display=False):\n",
    "    delta_n = {}\n",
    "    delta_d = {}\n",
    "    ip = {}\n",
    "    new_S = set(S)\n",
    "    total_weight = 2*sum([G[e[0]][e[1]]['weight'] for e in G.edges()])\n",
    "    num =  sum([G[e[0]][e[1]]['weight'] for e in G.edges() if cut(e[0], e[1], new_S)])\n",
    "    denom = sum([sum([G[u][v]['weight'] for v in G.neighbors(u)]) for u in new_S])\n",
    "    if num == 0:\n",
    "        return list(new_S)\n",
    "    curr_score = num/min(denom, total_weight-denom)\n",
    "    \n",
    "    def set_ip(u):\n",
    "        if min(denom + delta_d[u], total_weight - denom - delta_d[u]) == 0:\n",
    "            ip[u] = 1000\n",
    "        else:\n",
    "            ip[u] = (num + delta_n[u])/min(denom + delta_d[u], total_weight - denom - delta_d[u]) - num/min(denom, total_weight - denom)\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        d = sum([G[u][v]['weight'] for v in G.neighbors(u)])\n",
    "        if d == 0:\n",
    "            return [u]\n",
    "        c = sum([G[u][v]['weight'] for v in G.neighbors(u) if cut(u,v,new_S)])\n",
    "        delta_n[u] = d-2*c\n",
    "        if u in new_S:\n",
    "            delta_d[u] = -d\n",
    "        else:\n",
    "            delta_d[u] = d\n",
    "        set_ip(u)\n",
    "    #TODO\n",
    "    all_neg = False\n",
    "    iters = 0\n",
    "    \n",
    "    while (not all_neg) and (iters < len(G.nodes)):\n",
    "        best_potential = 0\n",
    "        best_index = None\n",
    "        for v in G.nodes():\n",
    "            if ip[v] < best_potential:\n",
    "                best_potential = ip[v]\n",
    "                best_index = v\n",
    "        if not best_index is None:\n",
    "            num += delta_n[best_index]\n",
    "            denom += delta_d[best_index]\n",
    "            for j in G.neighbors(best_index):\n",
    "                if cut(best_index,j,new_S):\n",
    "                    delta_n[j] += 2*G[best_index][j]['weight']\n",
    "                else:\n",
    "                    delta_n[j] -= 2*G[best_index][j]['weight']\n",
    "                set_ip(j)\n",
    "            delta_n[best_index] = -delta_n[best_index]\n",
    "            delta_d[best_index] = -delta_d[best_index]\n",
    "            set_ip(best_index)\n",
    "            if best_index in new_S:\n",
    "                new_S.remove(best_index)\n",
    "            else:\n",
    "                new_S.add(best_index)\n",
    "            #print(\"curr scores:\", num/min(denom, total_weight - denom))\n",
    "        else:\n",
    "            all_neg = True\n",
    "        iters += 1\n",
    "    if display:\n",
    "        print(\"sgreed+ score, \",  num/min(denom, total_weight - denom))\n",
    "    return list(new_S)\n",
    "\n",
    "def evaluate_sparsity(S, G):\n",
    "    total_weight = 2*sum([G[e[0]][e[1]]['weight'] for e in G.edges()])\n",
    "    num =  sum([G[e[0]][e[1]]['weight'] for e in G.edges() if cut(e[0], e[1], S)])\n",
    "    denom = sum([sum([G[u][v]['weight'] for v in G.neighbors(u)]) for u in S])\n",
    "    return num/min(denom, total_weight - denom)\n",
    "    \n",
    "def build_tree_sep(samples, method='greedy', subset = None, sim_thresh=0, p = None, qs = None):\n",
    "    assert method in ['greedy', 'egreedy', 'SDP', 'greedy+', 'spectral', 'sgreedy+']\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    else:\n",
    "        subset = list(subset)\n",
    "    subset = remove_duplicates(samples, subset)\n",
    "    T = nx.DiGraph()\n",
    "    for i in subset:\n",
    "        T.add_node(i)\n",
    "    def build_helper(S):\n",
    "        assert S, \"error, S = \"+ str(S)\n",
    "        if len(S) == 1:\n",
    "            return list(S)[0]\n",
    "        left_set = set()\n",
    "        if method == 'greedy':\n",
    "            left_set = greedy_cut(samples, subset=S)\n",
    "        elif method == 'egreedy':\n",
    "            left_set = egreedy_cut(samples, p, qs, subset=S)\n",
    "        elif method == 'SDP':\n",
    "            G = construct_connectivity_graph(samples, subset=S)\n",
    "            left_set = max_cut_heuristic(G, 3, 50)\n",
    "        elif method == 'greedy+':\n",
    "            G = construct_connectivity_graph(samples, subset=S)\n",
    "            left_set = greedy_cut(samples, subset=S)\n",
    "            left_set = improve_cut(G,left_set)\n",
    "        elif method == 'spectral':\n",
    "            G = construct_similarity_graph(samples, subset=S, threshold=sim_thresh)\n",
    "            left_set = spectral_split(G)\n",
    "            left_set = spectral_improve_cut(left_set, G)\n",
    "        elif method == 'sgreedy+':\n",
    "            G = construct_similarity_graph(samples, subset=S, threshold=sim_thresh)\n",
    "            left_set = spectral_improve_cut(greedy_cut(samples, subset=S) , G)\n",
    "\n",
    "        if len(left_set) == 0 or len(left_set) == len(S):\n",
    "            left_set = greedy_cut(samples, subset=S)\n",
    "        right_set = set()\n",
    "        for i in S:\n",
    "            if not i in left_set:\n",
    "                right_set.add(i)\n",
    "        root = len(T.nodes) - len(subset) + len(samples)\n",
    "        T.add_node(root)\n",
    "        left_child = build_helper(left_set)\n",
    "        right_child = build_helper(right_set)\n",
    "        T.add_edge(root, left_child)\n",
    "        T.add_edge(root, right_child)\n",
    "        return root\n",
    "    build_helper(subset)\n",
    "    return T\n",
    "\n",
    "def triplets_correct_sep(T, Tt, sample_size=5000):\n",
    "    TC = 0\n",
    "    sample_set = np.array([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    for a in range(sample_size):\n",
    "        chosen = np.random.choice(sample_set, 3, replace=False)\n",
    "        if outgroup2(chosen[0], chosen[1], chosen[2], T)[0] == outgroup2(chosen[0], chosen[1], chosen[2], Tt)[0]:\n",
    "            TC += 1\n",
    "    return TC/sample_size\n",
    "\n",
    "def outgroup2(i, j, k, T):\n",
    "    assert i != j and i != k and j != k, str(i) + ' ' + str(j) + ' ' + str(k) + ' not distinct'\n",
    "    \n",
    "#     Li = find_tree_lineage(i, T)\n",
    "#     Lj = find_tree_lineage(j, T)\n",
    "#     Lk = find_tree_lineage(k, T)\n",
    "\n",
    "    Li = [node for node in nx.ancestors(T, i)]\n",
    "    Lj = [node for node in nx.ancestors(T, j)]\n",
    "    Lk = [node for node in nx.ancestors(T, k)]\n",
    "    \n",
    "    ij_common = len(set(Li) & set(Lj))\n",
    "    ik_common = len(set(Li) & set(Lk))\n",
    "    jk_common = len(set(Lj) & set(Lk))\n",
    "    index = min(ij_common, ik_common, jk_common)\n",
    "\n",
    "    if ij_common == ik_common and ik_common == jk_common:\n",
    "        return None, index\n",
    "    if ij_common > ik_common and ij_common > jk_common:\n",
    "        return k, index\n",
    "    elif jk_common > ik_common and jk_common > ij_common:\n",
    "        return i, index\n",
    "    elif ik_common > ij_common and ik_common > jk_common:\n",
    "        return j, index\n",
    "\n",
    "def triplets_correct_stratified(T, Tt, sample_size=5000, min_size_depth = 20):\n",
    "    correct_class = defaultdict(int)\n",
    "    freqs = defaultdict(int)\n",
    "    sample_set = np.array([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    \n",
    "    for a in range(sample_size):\n",
    "        chosen = np.random.choice(sample_set, 3, replace=False)\n",
    "        out1, index = outgroup2(chosen[0], chosen[1], chosen[2], T)\n",
    "        out2, index2 = outgroup2(chosen[0], chosen[1], chosen[2], Tt)\n",
    "        correct_class[index] += (out1 == out2)\n",
    "        freqs[index] += 1\n",
    "        \n",
    "    tot_tp = 0\n",
    "    num_consid = 0\n",
    "    \n",
    "    for k in correct_class.keys():\n",
    "        if freqs[k] > min_size_depth:\n",
    "\n",
    "            num_consid += 1\n",
    "            tot_tp += correct_class[k] / freqs[k]\n",
    "\n",
    "    tot_tp /= num_consid\n",
    "    return tot_tp\n",
    "\n",
    "def get_colless(network):\n",
    "    root = [n for n in network if network.in_degree(n) == 0][0]\n",
    "    colless = [0]\n",
    "    colless_helper(network, root, colless)\n",
    "    n = len([n for n in network if network.out_degree(n) == 0 and network.in_degree(n) == 1]) \n",
    "    return colless[0], (colless[0] - n * np.log(n) - n * (np.euler_gamma - 1 - np.log(2)))/n\n",
    "\n",
    "def colless_helper(network, node, colless):\n",
    "    if network.out_degree(node) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        leaves = []\n",
    "        for i in network.successors(node):\n",
    "            leaves.append(colless_helper(network, i, colless))\n",
    "        colless[0] += abs(leaves[0] - leaves[1])\n",
    "        return sum(leaves)\n",
    "\n",
    "def triplets_correct_at_time_sep(T, Tt, method='all', bin_size = 10, sample_size=5000, sampling_depths=None):\n",
    "    sample_set = set([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    children = {}\n",
    "    num_triplets = {}\n",
    "    nodes_at_depth = {}\n",
    "\n",
    "    def find_children(node, total_time):\n",
    "        t = total_time + Tt.nodes[node]['parent_lifespan']\n",
    "        children[node] = []\n",
    "        if Tt.out_degree(node) == 0:\n",
    "            if node in sample_set:\n",
    "                children[node].append(node)\n",
    "            return\n",
    "\n",
    "        for n in Tt.neighbors(node):\n",
    "            find_children(n, t)\n",
    "            children[node] += children[n]\n",
    "\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        num_triplets[node] = len(children[L])*nCr(len(children[R]), 2) + len(children[R])*nCr(len(children[L]), 2)\n",
    "        if num_triplets[node] > 0:\n",
    "            bin_num = t//bin_size\n",
    "            \n",
    "            if bin_num in nodes_at_depth:\n",
    "                nodes_at_depth[bin_num].append(node)\n",
    "            else:\n",
    "                nodes_at_depth[bin_num] = [node]\n",
    "                \n",
    "    root = [n for n in Tt if Tt.in_degree(n) == 0][0]\n",
    "    find_children(root, 0)\n",
    "\n",
    "    def sample_at_depth(d):\n",
    "        denom = sum([num_triplets[v] for v in nodes_at_depth[d]])\n",
    "        node = np.random.choice(nodes_at_depth[d], 1, [num_triplets[v]/denom for v in nodes_at_depth[d]])[0]\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        if np.random.random() < (len(children[R])-1)/(len(children[R])+len(children[L])-2):\n",
    "            outgrp = np.random.choice(children[L], 1)[0]\n",
    "            ingrp = np.random.choice(children[R], 2, replace=False)\n",
    "        else:\n",
    "            outgrp = np.random.choice(children[R], 1)[0]\n",
    "            ingrp = np.random.choice(children[L], 2, replace=False)\n",
    "        return outgroup2(ingrp[0], ingrp[1], outgrp, T)[0] == outgrp\n",
    "\n",
    "    if not sampling_depths:\n",
    "        sampling_depths = [d for d in range(len(nodes_at_depth))]\n",
    "    if method == 'aggregate':\n",
    "        score = 0\n",
    "        freq = 0\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    freq += 1\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "        return score/(sample_size*freq)\n",
    "    elif method == 'all':\n",
    "        ret = ['NA'] * len(sampling_depths)\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    score = 0\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "                    ret[d] = score/sample_size\n",
    "        return np.array(ret)\n",
    "\n",
    "def triplets_correct_at_depth_sep(T, Tt, method='all', sample_size=5000, sampling_depths=None):\n",
    "    sample_set = set([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    children = {}\n",
    "    num_triplets = {}\n",
    "    nodes_at_depth = {}\n",
    "\n",
    "    def find_children(node, depth):\n",
    "        children[node] = []\n",
    "        if Tt.out_degree(node) == 0:\n",
    "            if node in sample_set:\n",
    "                children[node].append(node)\n",
    "            return\n",
    "\n",
    "        for n in Tt.neighbors(node):\n",
    "            find_children(n, depth+1)\n",
    "            children[node] += children[n]\n",
    "\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        num_triplets[node] = len(children[L])*nCr(len(children[R]), 2) + len(children[R])*nCr(len(children[L]), 2)\n",
    "        \n",
    "        if num_triplets[node] > 0:\n",
    "            if depth in nodes_at_depth:\n",
    "                nodes_at_depth[depth].append(node)\n",
    "            else:\n",
    "                nodes_at_depth[depth] = [node]\n",
    "                \n",
    "    root = [n for n in Tt if Tt.in_degree(n) == 0][0]\n",
    "    find_children(root, 0)\n",
    "\n",
    "    def sample_at_depth(d):\n",
    "        denom = sum([num_triplets[v] for v in nodes_at_depth[d]])\n",
    "        node = np.random.choice(nodes_at_depth[d], 1, [num_triplets[v]/denom for v in nodes_at_depth[d]])[0]\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        if np.random.random() < (len(children[R])-1)/(len(children[R])+len(children[L])-2):\n",
    "            outgrp = np.random.choice(children[L], 1)[0]\n",
    "            ingrp = np.random.choice(children[R], 2, replace=False)\n",
    "        else:\n",
    "            outgrp = np.random.choice(children[R], 1)[0]\n",
    "            ingrp = np.random.choice(children[L], 2, replace=False)\n",
    "        return outgroup2(ingrp[0], ingrp[1], outgrp, T)[0] == outgrp\n",
    "\n",
    "    if not sampling_depths:\n",
    "        sampling_depths = [d for d in range(len(nodes_at_depth))]\n",
    "        \n",
    "    if method == 'aggregate':\n",
    "        score = 0\n",
    "        freq = 0\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    freq += 1\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "        return score/(sample_size*freq)\n",
    "    elif method == 'all':\n",
    "        ret = ['NA'] * len(sampling_depths)\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    score = 0\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "                    ret[d] = score/sample_size\n",
    "        return np.array(ret) \n",
    "\n",
    "def eX(p, q, h):\n",
    "    if p == 0.5:\n",
    "        return 2*p*q*(h-1)\n",
    "    return (2*q*p*(1-(2-2*p)**h)/(2*p-1))\n",
    "\n",
    "def VarX(p, q, h):\n",
    "    def cross_terms(p, q, h):\n",
    "        if p == 0.5:\n",
    "            return p**2*q**2*sum([(h-d)**2 for d in range(h)])\n",
    "        s = 0\n",
    "        for d in range(h):\n",
    "            s += ((2-2*p)**d)*((2-2*p)**(h-d)-1)**2\n",
    "        return p**2*q**2*s/(2*p-1)**2\n",
    "    return 2*cross_terms(p,q,h) + eX(p,q,h) - eX(p,q,h)**2\n",
    "\n",
    "def eY(p, q, h):\n",
    "    return q*(1-(1-p)**h)\n",
    "\n",
    "def Var_Y(p,q,h):\n",
    "    Ey = q*(1-(1-p)**h)\n",
    "    s = 0\n",
    "    s += 2*(1-((1-p)/2)**h)/(1+p) - 4*(1-p)**h*(1-2**(-h)) + (2-2*p)*(1-p)**(2*h)*(1-(1/(2-2*p))**h)/(1-2*p)\n",
    "    Eyiyj = q**2*s/4\n",
    "    Eyiyi = p*q*(1-((1-p)/2)**h)/(1+p)\n",
    "    return 2*Eyiyj + Eyiyi - Ey**2\n",
    "\n",
    "def CoV(p, q, h):\n",
    "    s = 2*(2-2*p)**h*(1-2**(-h)) - (2-2*p)*(1-p)**h*((2-2*p)**h - 1)/(1-2*p) - (1-(1-p)**h)/p + h*(1-p)**h\n",
    "    twoExiyj = p*q**2*s/(1-2*p)\n",
    "    Ey = q*(1-(1-p)**h)\n",
    "    Ex = (2*q*p*(1-(2-2*p)**h)/(2*p-1))\n",
    "    return twoExiyj + Ey - Ex*Ey\n",
    "\n",
    "def generate_frequency_dict(samples, subset=None):\n",
    "    k = samples[0].num_chars\n",
    "    F = {}\n",
    "    for n in range(k):\n",
    "        F[n] = {}\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    for i in subset:\n",
    "        for j in range(k):\n",
    "            if samples[i].chars[j] in F[j]:\n",
    "                F[j][samples[i].chars[j]] += 1\n",
    "            else:\n",
    "                F[j][samples[i].chars[j]] = 1\n",
    "    return F\n",
    "\n",
    "def score_egreedy(samples, p, qs, subset = None):\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    F = generate_frequency_dict(samples, subset)\n",
    "    h = math.log2(len(subset))\n",
    "    total = 0\n",
    "    \n",
    "    if len(subset) > 1:\n",
    "        for char in F:\n",
    "            for state in F[char]:\n",
    "                if state == 0 or state == -1:\n",
    "                    continue\n",
    "                q = qs[char][str(state)]\n",
    "                if q > 0:\n",
    "                    total += (eX(p,q,h) + CoV(p,q,h)/Var_Y(p,q,h) * (F[char][state] - eY(p,q,h)))\n",
    "\n",
    "    return total\n",
    "\n",
    "def egreedy_cut(samples, p, qs, subset = None):\n",
    "    if not subset:\n",
    "        subset = list(range(len(samples)))\n",
    "    F = generate_frequency_dict(samples, subset)\n",
    "    k = samples[0].num_chars\n",
    "    min_score = 9223372036854775807\n",
    "    split_char = 0\n",
    "    split_state = 0\n",
    "    \n",
    "    for char in F:\n",
    "        for state in F[char]:\n",
    "            if state == 0 or state == -1:\n",
    "                continue\n",
    "                \n",
    "            S = set()\n",
    "            Sc = set()\n",
    "            missing = set()\n",
    "            #print(char, state)\n",
    "            for i in subset:\n",
    "                if samples[i].chars[char] == state:\n",
    "                    S.add(i)\n",
    "                elif samples[i].chars[char] == -1:\n",
    "                    missing.add(i)\n",
    "                else:\n",
    "                    Sc.add(i)\n",
    "            \n",
    "            if not Sc or not S:\n",
    "                continue\n",
    "                \n",
    "            score_S = score_egreedy(samples, p, qs, S)\n",
    "            score_Sc = score_egreedy(samples, p, qs, Sc)\n",
    "            \n",
    "#             print(score_S + score_Sc)\n",
    "            if score_S + score_Sc < min_score:\n",
    "                min_score = score_S + score_Sc\n",
    "                split_char = char\n",
    "                split_state = state\n",
    "    \n",
    "    if split_state == 0:\n",
    "        return random_nontrivial_cut(subset)\n",
    "    \n",
    "    S = set()\n",
    "    Sc = set()\n",
    "    missing = set()\n",
    "    #print(char, state)\n",
    "    for i in subset:\n",
    "        if samples[i].chars[split_char] == split_state:\n",
    "            S.add(i)\n",
    "        elif samples[i].chars[split_char] == -1:\n",
    "            missing.add(i)\n",
    "        else:\n",
    "            Sc.add(i)\n",
    "    \n",
    "    for i in missing:\n",
    "        s_score = 0\n",
    "        sc_score = 0\n",
    "        for j in S:\n",
    "            for l in range(k):\n",
    "                if samples[i].chars[l] > 0 and samples[i].chars[l] == samples[j].chars[l]:\n",
    "                    s_score += 1\n",
    "        for j in Sc:\n",
    "            for l in range(k):\n",
    "                if samples[i].chars[l] > 0  and samples[i].chars[l] == samples[j].chars[l]:\n",
    "                    sc_score += 1\n",
    "        if s_score/len(S) > sc_score/len(Sc):\n",
    "            S.add(i)\n",
    "        else:\n",
    "            Sc.add(i)\n",
    "            \n",
    "    if not Sc:\n",
    "        if len(S) == len(subset) or len(S) == 0:\n",
    "            print('One side of split empty')\n",
    "            print(F)\n",
    "            print(char, state, len(subset))\n",
    "        return S\n",
    "    \n",
    "#     if len(Sc) == 1 or len(S) == 1:\n",
    "#         print('split of size 1 chosen')\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['0.9878' '0.9834' '0.868' '0.8892' '0.8902' '0.7132' '0.7392' '0.6866'\n",
      " '0.6062' '0.661' 'NA' 'NA' 'NA' 'NA'] greedy\n",
      "1 ['0.5714' '0.8738' '0.8304' '0.9102' '0.751' '0.7748' '0.8028' '0.7816'\n",
      " '0.5696' '0.6742' '0.6808' '0.6714' '0.7832' 'NA' 'NA' 'NA'] greedy\n",
      "2 ['1.0' '0.6682' '0.5546' '1.0' '0.9606' '0.8246' '0.7498' '0.862' '0.6312'\n",
      " '0.6972' '0.73' '0.8272' '0.6994' '0.6524' 'NA' 'NA' 'NA' 'NA'] greedy\n",
      "3 ['1.0' '1.0' '0.7648' '0.7834' '0.7948' '0.8034' '0.8212' '0.6712'\n",
      " '0.7618' '0.7036' '0.7762' '0.7966' 'NA' 'NA' 'NA' 'NA'] greedy\n",
      "4 ['1.0' '1.0' '0.8868' '0.702' '0.8494' '0.8626' '0.8114' '0.827' '0.765'\n",
      " '0.6662' 'NA' 'NA' 'NA'] greedy\n",
      "5 ['1.0' '0.709' '0.633' '0.8356' '0.8568' '0.6366' '0.7118' '0.7128'\n",
      " '0.7416' '0.7656' '0.6902' 'NA' 'NA' 'NA' 'NA' 'NA' 'NA' 'NA'] greedy\n",
      "6 ['0.9348' '0.6788' '0.7812' '0.9072' '0.9804' '0.808' '0.7418' '0.8408'\n",
      " '0.7446' '0.8206' 'NA' 'NA' 'NA' 'NA' 'NA' 'NA'] greedy\n",
      "7 ['1.0' '1.0' '0.9938' '0.7832' '0.8904' '0.834' '0.784' '0.7606' '0.8008'\n",
      " '0.736' '0.6544' '0.74' '0.7046' 'NA' 'NA' 'NA' 'NA'] greedy\n",
      "8 ['1.0' '1.0' '0.919' '0.95' '0.9698' '0.8292' '0.7378' '0.7624' '0.611'\n",
      " '0.711' '0.669' '0.517' '0.5724' '0.781' 'NA' 'NA' 'NA' 'NA' 'NA' 'NA'] greedy\n",
      "9 ['0.7504' '0.9566' '0.9' '0.7762' '0.8646' '0.7802' '0.787' '0.7822'\n",
      " '0.7698' '0.6524' '0.5936' '0.759' '0.6488' '0.7142' '0.8044' '1.0'\n",
      " '0.8634' '0.7484' '0.4076' 'NA' 'NA' 'NA'] greedy\n"
     ]
    }
   ],
   "source": [
    "folder = \"400cells_drop_10_m\"\n",
    "path = \"/data/yosef2/users/richardz/projects/benchmarking/\" + folder + \"/\"\n",
    "nums = []\n",
    "#     triplets = []\n",
    "triplets_new = []\n",
    "colless = []\n",
    "types = []\n",
    "methods = []\n",
    "\n",
    "# for sim_thresh in [0, 1, 2]:\n",
    "\n",
    "    #     for method in [\"greedy\", \"egreedy\", \"sgreedy+\", \"spectral\", \"greedy+\", \"SDP\"]:\n",
    "for method in [\"greedy\"]:\n",
    "    for num in range(0, 10):\n",
    "        dropout_cm = pd.read_csv(path + \"dropout_cm\" + str(num) + \".txt\", sep = '\\t', index_col = 0)\n",
    "        dropout_cm = dropout_cm.applymap(str)\n",
    "\n",
    "        samples = []\n",
    "        for index, row in dropout_cm.iterrows():\n",
    "            node = Node(list(row), 1001, parent=None, left=None, right=None)\n",
    "            samples.append(node)\n",
    "\n",
    "        subset = list(range(len(samples)))\n",
    "        prune_samples = remove_duplicates(samples, subset)\n",
    "\n",
    "        node_map = {}\n",
    "        for i in prune_samples:\n",
    "            node_map[i] = list(dropout_cm.iloc[i,:])\n",
    "\n",
    "        net = pic.load(open(path + \"dropout_net\" + str(num) + \".pkl\", 'rb'))\n",
    "        ground = net.network\n",
    "        leaves = [n for n in ground if ground.out_degree(n) == 0 and ground.in_degree(n) == 1]\n",
    "        ground_net_map = {}\n",
    "        for i in node_map:\n",
    "            for j in leaves:\n",
    "                if node_map[i] == j.char_vec:\n",
    "                    ground_net_map[j] = i\n",
    "                    break\n",
    "        ground = nx.relabel_nodes(ground, ground_net_map)\n",
    "\n",
    "        dropout_cm = dropout_cm.replace(\"-\", -1)\n",
    "        dropout_cm = dropout_cm.replace(\"*\", -1)\n",
    "        dropout_cm = dropout_cm.apply(pd.to_numeric)\n",
    "\n",
    "        samples = []\n",
    "        for index, row in dropout_cm.iterrows():\n",
    "            node = Node(list(row), 1001, parent=None, left=None, right=None)\n",
    "            samples.append(node)\n",
    "\n",
    "    #     sample_map = {}\n",
    "    #     for i in range(len(samples)):\n",
    "    #         sample_map[i] = samples[i]\n",
    "        if method == 'egreedy':\n",
    "            p = 0.007550225842780328\n",
    "#                 p = np.log(1 - (mut/100))/(-1 * 100)\n",
    "            priors = pic.load(open(path + \"priors\" + str(num) + \".pkl\", 'rb'))\n",
    "            recon = build_tree_sep(samples, method=\"egreedy\", p = p, qs = priors)\n",
    "#         elif method == 'spectral' or method == 'sgreedy+':\n",
    "#             subsample = []\n",
    "#             for i in range(2**9):\n",
    "#                 if np.random.random() < 0.2:\n",
    "#                     subsample.append(i)\n",
    "            recon = build_tree_sep(samples, subset = subsample, method=method, sim_thresh = sim_thresh)\n",
    "        else:\n",
    "            recon = build_tree_sep(samples, method=method)\n",
    "#             trip = triplets_correct_stratified(recon, ground)\n",
    "        trip2 = triplets_correct_at_depth_sep(recon, ground, 'all')\n",
    "        print(num, trip2, method)\n",
    "#             triplets.append(trip)\n",
    "        triplets_new.append(trip2)\n",
    "        nums.append(num)\n",
    "        colless.append(get_colless(ground)[0])\n",
    "        types.append(folder)\n",
    "        methods.append(method)\n",
    "\n",
    "data = [nums, triplets_new, colless, methods, types]\n",
    "df = pd.DataFrame(data)\n",
    "df = df.T\n",
    "df = df.rename(columns = {0: 'Run', 1: 'TripletsCorrect', 2:'Colless', 3:'Method', 4: 'Type'})\n",
    "# df.to_csv(path + 'methods_triplets.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "16\n",
      "18\n",
      "16\n",
      "13\n",
      "18\n",
      "16\n",
      "17\n",
      "20\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "for i in triplets_new:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"400cells_base_m\"\n",
    "path = \"/data/yosef2/users/richardz/projects/benchmarking/\" + folder + \"/\"\n",
    "net = pic.load(open(path + \"dropout_net\" + str(0) + \".pkl\", 'rb'))\n",
    "ground = net.network\n",
    "\n",
    "def triplets_correct_at_time_sep(T, Tt, method='all', bin_size = 10, sample_size=5000, sampling_depths=None):\n",
    "    sample_set = set([v for v in T.nodes() if T.in_degree(v) == 1 and T.out_degree(v) == 0])\n",
    "    children = {}\n",
    "    num_triplets = {}\n",
    "    nodes_at_depth = {}\n",
    "\n",
    "    def find_children(node, total_time):\n",
    "        t = total_time + Tt.nodes[node]['parent_lifespan']\n",
    "        children[node] = []\n",
    "        if Tt.out_degree(node) == 0:\n",
    "            if node in sample_set:\n",
    "                children[node].append(node)\n",
    "            return\n",
    "\n",
    "        for n in Tt.neighbors(node):\n",
    "            find_children(n, t)\n",
    "            children[node] += children[n]\n",
    "\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        num_triplets[node] = len(children[L])*nCr(len(children[R]), 2) + len(children[R])*nCr(len(children[L]), 2)\n",
    "        if num_triplets[node] > 0:\n",
    "            bin_num = t//bin_size\n",
    "            \n",
    "            if bin_num in nodes_at_depth:\n",
    "                nodes_at_depth[bin_num].append(node)\n",
    "            else:\n",
    "                nodes_at_depth[bin_num] = [node]\n",
    "                \n",
    "    root = [n for n in Tt if Tt.in_degree(n) == 0][0]\n",
    "    find_children(root, 0)\n",
    "\n",
    "    def sample_at_depth(d):\n",
    "        denom = sum([num_triplets[v] for v in nodes_at_depth[d]])\n",
    "        node = np.random.choice(nodes_at_depth[d], 1, [num_triplets[v]/denom for v in nodes_at_depth[d]])[0]\n",
    "        L, R = list(Tt.neighbors(node))[0], list(Tt.neighbors(node))[1]\n",
    "        if np.random.random() < (len(children[R])-1)/(len(children[R])+len(children[L])-2):\n",
    "            outgrp = np.random.choice(children[L], 1)[0]\n",
    "            ingrp = np.random.choice(children[R], 2, replace=False)\n",
    "        else:\n",
    "            outgrp = np.random.choice(children[R], 1)[0]\n",
    "            ingrp = np.random.choice(children[L], 2, replace=False)\n",
    "        return outgroup2(ingrp[0], ingrp[1], outgrp, T)[0] == outgrp\n",
    "\n",
    "    if not sampling_depths:\n",
    "        sampling_depths = [d for d in range(len(nodes_at_depth))]\n",
    "    if method == 'aggregate':\n",
    "        score = 0\n",
    "        freq = 0\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    freq += 1\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "        return score/(sample_size*freq)\n",
    "    elif method == 'all':\n",
    "        ret = ['NA'] * len(sampling_depths)\n",
    "        for d in sampling_depths:\n",
    "            if d in nodes_at_depth:\n",
    "                max_children = 0\n",
    "                for i in nodes_at_depth[d]:\n",
    "                    if len(children[i]) > max_children:\n",
    "                        max_children = len(children[i])\n",
    "                if max_children > 10:\n",
    "                    score = 0\n",
    "                    for a in range(sample_size):\n",
    "                        score += int(sample_at_depth(d))\n",
    "                    ret[d] = score/sample_size\n",
    "        return np.array(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.73724 avg\n",
      "1 0.6108923076923077 avg\n",
      "2 0.6826142857142857 avg\n",
      "3 0.7185166666666667 avg\n",
      "4 0.71582 avg\n",
      "5 0.6319090909090909 avg\n",
      "6 0.7618 avg\n",
      "7 0.7260615384615384 avg\n",
      "8 0.6434857142857143 avg\n",
      "9 0.6576 avg\n",
      "0 0.73216 avg_h\n",
      "1 0.5974615384615385 avg_h\n",
      "2 0.6939285714285715 avg_h\n",
      "3 0.7181833333333333 avg_h\n",
      "4 0.71772 avg_h\n",
      "5 0.6369818181818182 avg_h\n",
      "6 0.74784 avg_h\n",
      "7 0.7282615384615385 avg_h\n",
      "8 0.6418714285714285 avg_h\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-67bc059b2e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelabel_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_net_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mleaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mground\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_degree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_degree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mtrip2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplets_correct_at_depth_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aggregate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrip2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m#             triplets.append(trip)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mtriplets_correct_at_depth_sep\u001b[0;34m(T, Tt, method, sample_size, sampling_depths)\u001b[0m\n\u001b[1;32m    790\u001b[0m                     \u001b[0mfreq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_at_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36msample_at_depth\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0moutgrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mingrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutgroup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingrp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutgrp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msampling_depths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36moutgroup2\u001b[0;34m(i, j, k, T)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mLi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mancestors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0mLj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mancestors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mLk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mancestors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0mij_common\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/networkx/algorithms/dag.py\u001b[0m in \u001b[0;36mancestors\u001b[0;34m(G, source)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkXError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The node %s is not in the graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0manc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/generic.py\u001b[0m in \u001b[0;36mshortest_path_length\u001b[0;34m(G, source, target, weight, method)\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0;31m# to be reversed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0mpath_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_source_shortest_path_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                     \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dijkstra'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0mpath_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_source_dijkstra_path_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36msingle_source_shortest_path_length\u001b[0;34m(G, source, cutoff)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mnextlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_single_shortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36m_single_shortest_path_length\u001b[0;34m(adj, firstlevel, cutoff)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mseen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlevel\u001b[0m  \u001b[0;31m# set the level of vertex v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mnextlevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add neighbors of v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yosef2/users/richardz/anaconda2/envs/python3_6/lib/python3.6/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0mKeysView\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/networkx/classes/coreviews.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atlas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atlas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for drop in [10, 20, 30, 40, 50]:\n",
    "    folder = \"400cells_drop_\" + str(drop) + \"_m\"\n",
    "    path = \"/data/yosef2/users/richardz/projects/benchmarking/\" + folder + \"/\"\n",
    "    nums = []\n",
    "#     triplets = []\n",
    "    triplets_new = []\n",
    "    colless = []\n",
    "    types = []\n",
    "    methods = []\n",
    "    \n",
    "    for method in (\"avg\", \"avg_h\", \"lookahead2\", \"lookahead_h2\", \"knn_h\", \"knn\"):\n",
    "    \n",
    "        for num in range(0, 10):\n",
    "            dropout_cm = pd.read_csv(path + \"/dropout_cm\" + str(num) + \".txt\", sep = '\\t', index_col = 0)\n",
    "            dropout_cm = dropout_cm.applymap(str)\n",
    "\n",
    "            recon = pic.load(open(path + method + \"/dropout_cm\" + str(num) + \"_\" + method + \".pkl\", 'rb'))\n",
    "#             reconpp = recon.post_process(dropout_cm)\n",
    "#             recon = reconpp.network\n",
    "            recon = recon.network\n",
    "            nodes = list(recon.nodes())\n",
    "            for i in nodes:\n",
    "                if i.is_target:\n",
    "                    targets += 1\n",
    "                    if recon.out_degree(i) != 0:\n",
    "                        i.is_target = False\n",
    "                        new_node = Cass_Node(i.name, i.get_character_vec(), is_target=True)\n",
    "                        recon.add_node(new_node)\n",
    "                        recon.add_edge(i, new_node)\n",
    "\n",
    "            leaves = [n for n in recon if recon.out_degree(n) == 0 and recon.in_degree(n) == 1]\n",
    "\n",
    "            node_map = {}\n",
    "            recon_map = {}\n",
    "            for i in range(len(leaves)):\n",
    "                node_map[i] = leaves[i].char_vec\n",
    "                recon_map[leaves[i]] = i\n",
    "\n",
    "            recon = nx.relabel_nodes(recon, recon_map)\n",
    "\n",
    "            net = pic.load(open(path + \"/dropout_net\" + str(num) + \".pkl\", 'rb'))\n",
    "            ground = net.network\n",
    "            leaves = [n for n in ground if ground.out_degree(n) == 0 and ground.in_degree(n) == 1]\n",
    "\n",
    "            ground_net_map = {}\n",
    "            for i in node_map:\n",
    "                for j in leaves:\n",
    "                    if node_map[i] == j.char_vec:\n",
    "                        ground_net_map[j] = i\n",
    "                        break\n",
    "\n",
    "            ground = nx.relabel_nodes(ground, ground_net_map)\n",
    "            leaves = [n for n in ground if ground.out_degree(n) == 0 and ground.in_degree(n) == 1]\n",
    "            trip2 = triplets_correct_at_depth_sep(recon, ground, 'aggregate')\n",
    "            print(num, trip2, method)\n",
    "    #             triplets.append(trip)\n",
    "            triplets_new.append(trip2)\n",
    "            nums.append(num)\n",
    "            colless.append(get_colless(ground)[0])\n",
    "            types.append(folder)\n",
    "            methods.append(method)\n",
    "        \n",
    "    data = [nums, triplets_new, colless, methods, types]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.T\n",
    "    df = df.rename(columns = {0: 'Run', 1: 'TripletsCorrect', 2:'Colless', 3:'Method', 4: 'Type'})\n",
    "for i in triplets_new:\n",
    "    print(len(i))\n",
    "#     df.to_csv(path + 'heritable_methods_triplets.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321 321 321\n",
      "340 340 340\n",
      "352 351 352\n",
      "352 352 352\n",
      "325 325 325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5067904e53d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'greedy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mlen1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecon\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_degree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_tree_sep\u001b[0;34m(samples, method, subset, sim_thresh, p, qs)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mbuild_helper\u001b[0;34m(S)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mleft_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'greedy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mleft_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'egreedy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mleft_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0megreedy_cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-979b51ba6d68>\u001b[0m in \u001b[0;36mgreedy_cut\u001b[0;34m(samples, subset)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder = \"400cells_drop_10_m\"\n",
    "path = \"/data/yosef2/users/richardz/projects/benchmarking/\" + folder + \"/\"\n",
    "\n",
    "for num in range(50):\n",
    "\n",
    "    dropout_cm = pd.read_csv(path + \"dropout_cm\" + str(num) + \".txt\", sep = '\\t', index_col = 0)\n",
    "    dropout_cm = dropout_cm.applymap(str)\n",
    "\n",
    "    samples = []\n",
    "    for index, row in dropout_cm.iterrows():\n",
    "        node = Node(list(row), 1001, parent=None, left=None, right=None)\n",
    "        samples.append(node)\n",
    "\n",
    "    subset = list(range(len(samples)))\n",
    "    prune_samples = remove_duplicates(samples, subset)\n",
    "\n",
    "    node_map = {}\n",
    "    for i in prune_samples:\n",
    "        node_map[i] = list(dropout_cm.iloc[i,:])\n",
    "\n",
    "    net = pic.load(open(path + \"dropout_net\" + str(num) + \".pkl\", 'rb'))\n",
    "    ground = net.network\n",
    "    leaves = [n for n in ground if ground.out_degree(n) == 0 and ground.in_degree(n) == 1]\n",
    "    ground_net_map = {}\n",
    "    for i in node_map:\n",
    "        for j in leaves:\n",
    "            if node_map[i] == j.char_vec:\n",
    "                ground_net_map[j] = i\n",
    "                break\n",
    "    ground = nx.relabel_nodes(ground, ground_net_map)\n",
    "\n",
    "    dropout_cm = dropout_cm.replace(\"-\", -1)\n",
    "    dropout_cm = dropout_cm.replace(\"*\", -1)\n",
    "    dropout_cm = dropout_cm.apply(pd.to_numeric)\n",
    "\n",
    "    samples = []\n",
    "    for index, row in dropout_cm.iterrows():\n",
    "        node = Node(list(row), 1001, parent=None, left=None, right=None)\n",
    "        samples.append(node)\n",
    "\n",
    "    recon = build_tree_sep(samples, method='greedy')\n",
    "    len1 = len([n for n in recon if recon.out_degree(n) == 0])\n",
    "    \n",
    "    recon = pic.load(open(path + \"avg/dropout_cm\" + str(num) + \"_avg.pkl\", 'rb'))\n",
    "    recon = recon.network\n",
    "    targets = 0\n",
    "    nodes = list(recon.nodes())\n",
    "    for i in nodes:\n",
    "        if i.is_target:\n",
    "            targets += 1\n",
    "            if recon.out_degree(i) != 0:\n",
    "                i.is_target = False\n",
    "                new_node = Cass_Node(i.name, i.get_character_vec(), is_target=True)\n",
    "                recon.add_node(new_node)\n",
    "                recon.add_edge(i, new_node)\n",
    "#     reconpp = recon.post_process(dropout_cm)\n",
    "#     recon = reconpp.network\n",
    "\n",
    "    len2 = len([n for n in recon if recon.out_degree(n) == 0])\n",
    "    \n",
    "    \n",
    "    print(len(prune_samples), len1, len2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon = pic.load(open(path + \"avg/dropout_cm0_avg.pkl\", 'rb'))\n",
    "recon = recon.network\n",
    "\n",
    "leaves = [n for n in recon if recon.out_degree(n) == 0]\n",
    "len(leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
